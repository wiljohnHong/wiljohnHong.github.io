{"pages":[],"posts":[{"title":"hexo配置细节备忘","text":"基本流程 hexo建站基本配置直接参照官网步骤，配置主题为Minos。 公式支持 网上广为流传的替换hexo默认渲染为hexo-renderer-kramed的方法，仍然会存在行内公式含两个引号时中间部分被渲染为斜体的问题。 最后参照Hexo 书写 LaTeX 公式时的一些问题及解决方法，替换默认渲染为hexo-renderer-pandoc。 -- 注意事先去pandoc官方下载最新版本的pandoc，否则直接使用apt install pandoc可能在run服务时因为版本过低出现pandoc: Unknown extension: smart的问题。 图像居中 为了美观（强迫症），修改themes/source/css/style.scss中display属性为block，添加属性text-align: center。","link":"/2018/01/12/hexo-install/"},{"title":"Calculus of Variations","text":"Calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maximum and minimum of functionals. Introduction Functional is the function's function, in other words, functional takes a function as input and outputs a real number, or more mathematically, it refers to a mapping from a space \\(X\\) (commonly \\(X\\) is a space of functions) into the real numbers. In the calculus of variations, we seek a functional \\(f(x)\\) that maximizes (or minimizes) a functional \\(F[f]\\). For example, as we all know, the shortest curve between two fixed points is the straight line, and this can be proved using calculus of variations. Example 1: Shortest Curve To find out the shortest curve \\(y(x)\\) between two fixed points \\((a_1, b_1)\\) and \\((a_2, b_2)\\), we first express the length of the curve in the form of integration \\[ F[y] = \\int_{a_1}^{a_2} \\sqrt{1+[y&#39;(x)]^2} dx \\] where we have the boundary conditions of \\(y(x)\\) \\[ y(a_1) = b_1,\\ y(a_2) = b_2 \\] Suppose \\(y(x)\\) is twice continuously differentiable. Now if the functional \\(F\\) attains a local minimum at \\(y=f\\), consider \\(\\eta(x)\\) to be an arbitrary function that has at least one derivative and vanishes at end points \\(a_1\\) and \\(a_2\\), i.e. \\(\\eta(a_1) = 0\\) and \\(\\eta(a_2)=0\\), then we have \\[ F[f] \\leq F[f + \\epsilon\\eta] \\] where \\(\\epsilon\\) can be any number close to \\(0\\), here the term \\(\\epsilon \\eta\\) is called the variation of the function \\(f\\) and is denoted by \\(\\delta f\\). We can view \\(F[f+\\epsilon \\eta]\\) as a function of \\(\\epsilon\\) and denote it as \\(J(\\epsilon)\\), then \\(J&#39;(0) = 0\\) must be true for any arbitrary choice of \\(\\eta\\) since the functional \\(F[y]\\) achieves minimum when \\(y=f\\). By calculating \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\left. \\frac{d}{d\\epsilon}F[f+\\epsilon\\eta] \\right|_{\\epsilon=0} \\\\ &amp;= \\left. \\frac{d}{d\\epsilon} \\int_{a_1}^{a_2} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}dx \\right|_{\\epsilon=0} \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{d}{d\\epsilon} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{[f&#39;(x) + \\epsilon\\eta&#39;(x)]\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx \\end{aligned} \\] We get \\[ \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx = 0 \\] which means that in differentiable function space, \\(F[y]\\)'s derivatives in all directions (all \\(\\eta\\)) are zero. Since \\(y(x)\\) is twice continuously differentiable, we can further integrate the left hand side of above equation by parts \\[ \\begin{aligned} \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx &amp;= \\left. \\eta(x) \\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right|_{a_1}^{a_2} -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx \\\\ &amp;= -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx\\ \\ \\ \\text{(since $\\eta(a_1) = 0$ and $\\eta(a_2) = 0$)} \\end{aligned} \\] then now we have \\[ \\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx = 0 \\] Since \\(\\eta(x)\\) is arbitrary, imagine choosing a perturbation \\(\\eta(x)\\) that is zero everywhere except in the neighborhood of a point \\(\\hat{x}\\), in which case the functional derivative must be zero at \\(x=\\hat{x}\\), otherwise the integral will not be equal to zero. However, because this must be true for every choice of \\(\\hat{x}\\), the functional derivative must vanish for all values of \\(x\\), that is \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = 0 \\] This is called the fundamental lemma of calculus of variations. Follow this result we have \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = \\frac{f&#39;&#39;(x)}{(1 + [f&#39;(x)]^2)^{\\frac{3}{2}}} = 0 \\] So \\[ f&#39;&#39;(x)=0 \\] Euler–Lagrange Equation Now consider the more general case, for the functional \\[ F[y] = \\int_{x_1}^{x_2} L(x,y(x), y&#39;(x)) dx \\] Again suppose the functional attains a local minimum at \\(y=f\\), and define \\(J(\\epsilon) = F[f+\\epsilon\\eta]\\). Then we have \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\int_{x_1}^{x_2} \\left. \\frac{dL}{d\\epsilon} \\right|_{\\epsilon=0}dx \\\\ &amp;= \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f}\\eta +\\frac{\\partial L}{\\partial f&#39;}\\eta&#39; \\right) dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx +\\left.\\frac{\\partial L}{\\partial f&#39;}\\eta \\ \\right|_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\eta \\left(\\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\right) dx \\end{aligned} \\] According to the fundamental lemma of calculus of variations, we get \\[ \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] = 0 \\] which is called the Euler–Lagrange equation. The left hand side of this equation is called the functional derivative of \\(F[f]\\), denoted as \\[ \\frac{\\delta J}{\\delta f} = \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\] References [1] Wikipedia: Calculus of variations [2] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix D. New York: Springer.","link":"/2018/04/18/variations/"},{"title":"Lagrange Multiplier and KKT Conditions","text":"In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maximum and minimum of a function subject to equality constraints, while KKT conditions solve the problems with inequality constraints. Naive Approach Consider the problem of finding the maximum of a function \\(f(x_1, x_2)\\) subject to a constraint relating \\(x_1\\) and \\(x_2\\), which we write in the form \\[ g(x_1, x_2) = 0 \\] One approach would be to solve the above equation to express \\(x_2\\) as a function of \\(x_1\\) in the form of \\(x_2 = h(x_1)\\), then apply differentiation to \\(f(x_1, h(x_1))\\) w.r.t. \\(x_1\\) in the usual way. However, there are at least two drawbacks of this simple approach: Sometimes difficult or even impossible to find out an expression like \\(x_2 = h(x_1)\\) Spoil the natural symmetric between these variables A more elegant way is introducing a parameter \\(\\lambda\\) called a Lagrange multiplier. Lagrange Multiplier An Intuitive Understanding Now consider in a \\(D\\)-dimensional space where we need to \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = 0 \\] Geometrically, the constraint \\(g(\\mathbf{x}) = 0\\) represents a \\((D-1)\\)-dimensional surface, and what's worth noting is that at any point on the constraint surface, the gradient \\(\\nabla g(\\mathbf{x})\\) will be orthogonal to the surface. To see this, consider the total differential \\(dg = \\nabla g(\\mathbf{x})^T d \\mathbf{x}\\), then if the point \\(\\mathbf{x}+d\\mathbf{x}\\) also lies on the constraint surface, then \\(dg=0\\), so we have \\(\\nabla g(\\mathbf{x})^T d\\mathbf{x} = 0\\). Next, the point \\(\\mathbf{x}^*\\) on the constraint surface that maximize \\(f(\\mathbf{x})\\) must have the property that \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), in other words, \\(\\nabla g(\\mathbf{x}^*)\\) is orthogonal to the constraint surface, because otherwise we could increase the value of \\(f(\\mathbf{x})\\) by moving a short distance along the constraint surface. Personally, I think it's quite similar to the force resolution, where we can view the point \\(\\mathbf{x}\\) as a ball on its orbit -- the constraint surface. A force, \\(\\nabla f(\\mathbf{x})\\), is applied on it and the ball will finally moves up to a place where no force can be resolved on the tangent plane of the place. Since \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), there must exist a parameter \\(\\lambda\\) such that \\[ \\nabla f(\\mathbf{x}^*) +\\lambda \\nabla g(\\mathbf{x^*}) = 0 \\] where \\(\\lambda\\neq 0\\) is known as Lagrange multiplier. For representation convenience, we can introduce the Lagrangian function defined by \\[ L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) \\] By setting \\(\\nabla_{\\mathbf{x}} L=\\mathbf{0}\\) together with the constraint \\(g(\\mathbf{x}) = 0\\), we can get all the necessary conditions (but not sufficient) when \\(\\mathbf{x}\\) is an extreme point on the constraint surface. Example 1 \\[ \\max \\ 1-x_1^2 - x_2^2 \\\\ \\text{s.t.} \\ \\ x_1 + x_2 -1 =0 \\] The corresponding Lagrange function is given by \\(L(\\mathbf{x}, \\lambda) = 1-x_1^2 - x_2^2 + \\lambda (x_1 + x_2 -1)\\), then set \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_1} &amp;= -2x_1 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial x_2} &amp;= -2x_2 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial \\lambda} &amp;= x_1 + x_2 -1 = 0 \\end{aligned} \\] By solving the system of equations we get the stationary point on the constraint surface is \\((\\frac{1}{2},\\frac{1}{2})\\), and it can be verified that it is a maximum point by checking nearby points or using second-order sufficient conditions (see further reading 2). KKT Conditions So far we have considered the optimization problem with equality constraint only. Now consider such problem with inequality constraint \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) \\geq 0 \\] The solution of this problem can be classified into two kinds: When the constraint is active, that is when the stationary point lies on the boundary \\(g(\\mathbf{x})=0\\), the problem is just analogous to the one discussed previously and corresponds to a stationary point with \\(\\lambda \\neq 0\\). Note that now the sign of \\(\\lambda\\) is crucial, since \\(f(\\mathbf{x})\\) will only be at a maximum if \\(\\nabla f(\\mathbf{x})\\) is away from the region \\(g(\\mathbf{x})&gt;0\\). So further we have the corresponding \\(\\lambda &gt; 0\\). When the constraint is inactive, the stationary point lies in the region \\(g(\\mathbf{x})&gt;0\\), with corresponding \\(\\lambda = 0\\). Again for the convenience of representation, note that for both cases, the product \\(\\lambda g(\\mathbf{x})=0\\), thus the solution can be obtained by maximizing the Lagrange function w.r.t. \\(\\mathbf{x}\\) s.t. the conditions \\[ \\begin{aligned} g(\\mathbf{x}) &amp;\\geq 0 &amp; &amp; \\text{(constriant function)} \\\\ \\lambda &amp;\\geq 0&amp;&amp; \\text{(gradient direction)} \\\\ \\lambda g(\\mathbf{x})&amp;=0&amp;&amp; \\text{(complementary slackness condition)} \\end{aligned} \\] These are known as Karush-Kuhn-Tucker(KKT) conditions. If we need to apply KKT conditions to minimization problems, just change the sign of \\(\\lambda\\) in Lagrange function to keep the gradient direction reversed, i.e. \\(L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\\) ###Multiple Equality and Inequality Constraints Finally, it's now easy to extend the method to the case of multiple equality and inequality constraints \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\ \\ \\ \\ \\ \\ h_j(\\mathbf{x}) =0, \\ j=1,...,l \\] Define the Lagrange function to be \\[ L(\\mathbf{x},\\mathbf{v},\\mathbf{w}) = f(\\mathbf{x}) + \\sum_{i=1}^m w_i g_i(\\mathbf{x}) + \\sum_{j=1}^l v_j h_j(\\mathbf{x}) \\] Now the necessary conditions when a point \\(\\mathbf{x}\\) becomes the local optimum is \\[ \\left\\{\\begin{aligned} &amp; \\nabla_x L(\\mathbf{x},\\mathbf{w},\\mathbf{v}) = 0 \\\\&amp;g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\&amp;h_j(\\mathbf{x}) = 0, \\ j=1,...,l \\\\&amp; w_i \\geq 0,\\ i=1,...,m \\\\&amp; w_i g(\\mathbf{x})=0,\\ i=1,...,m \\end{aligned}\\right. \\] Example 2 \\[ \\min \\ x_1^2 - x_2 - 3x_3 \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = -x_1 - x_2 - x_3 \\geq 0 \\\\ \\ \\ \\ \\ \\ \\ h(\\mathbf{x}) = x_1^2 + 2x_2 - x_3 = 0 \\] Write down Lagrange function for minimization problem \\[ L(\\mathbf{x}, w, v) = (x_1^2 - x_2 - 3x_3) - w(-x_1 - x_2 - x_3) - v(x_1^2 + 2x_2 - x_3 ) \\] As described above, the first-order necessary condition of optimum is \\[ \\left\\{\\begin{aligned} &amp;L_{x_1}&#39; = 2x_1 + w - 2vx_1 =0 \\\\ &amp;L_{x_2}&#39; = -1 + w - 2v=0 \\\\ &amp;L_{x_3}&#39; = -3 + w +v=0 \\\\ &amp;-x_1 - x_2 - x_3 \\geq 0 \\\\ &amp; x_1^2 + 2x_2 - x_3 = 0 \\\\&amp;w\\geq 0 \\\\&amp; w(-x_1 - x_2 - x_3) = 0 \\end{aligned}\\right. \\] The solution is \\(\\mathbf{x}^* = (-\\frac{7}{2}, -\\frac{35}{12}, \\frac{77}{12}), \\ w = \\frac{7}{3}, \\ v=\\frac{2}{3}\\). Further Reading Lagrange Multipliers Can Fail To Determine Extrema In single variable case, when the gradient of the constraint function \\(\\nabla g(\\mathbf{x})=0\\), the simple Lagrangian multiplier discussed above doesn't work. e.g. \\(\\min \\ x \\text{, s.t.} \\ \\ g(x,y) = y^2 + x^4 - x^3=0\\). http://web.cs.iastate.edu/~cs577/handouts/lagrange-multiplier.pdf Explain Lagrange multiplier in multi-constraint case in detail, and give the second-order sufficient conditions, which use the definitive property of Lagrangian function's Hessian on the tangent space to verify if a stationary point is also an extreme point in a more mathematical way. Besides, it avoids discussing the cases like that mentioned in the above article by introducing the concept of regular point. Tangent space at ^* on a surface defined by (a) one constraint (shown as a tangent plane) and (b) two constraints (shown as a tangent line). According to the second-order sufficient conditions mentioned in further reading 2, we can verifying the solution as optimum in example 2 by first calculating the Hessian at this point \\[ \\nabla_{\\mathbf{x}}^2L(\\mathbf{x}^*, w, v) = \\begin{bmatrix} \\frac{2}{3} &amp;0&amp; 0 \\\\0&amp;0&amp;0\\\\0&amp;0&amp;0 \\end{bmatrix} \\] Since both the two constraints are active at \\(\\mathbf{x^*}\\), we can solving the systems of equations below to get the expression of the tangent space at this point \\[ \\left\\{ \\begin{aligned} \\nabla g(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\\\ \\nabla h(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\end{aligned} \\right. \\] where \\(\\mathbf{d} = (d_1, d_2, d_3)^T\\) stands for any feasible direction on the tangent space, and \\(g(\\mathbf{x}^*) = (-1,-1,-1)^T\\), \\(h(\\mathbf{x}^*) = (-7,2,-1)\\). By solving that we have \\(\\mathbf{d} = (d_1,2d_1,-3 d_1)^T\\). Because \\(\\mathbf{d}^T\\nabla^2_{\\mathbf{x}}L(\\mathbf{x}^*, w, v) \\mathbf{d}=\\frac{2}{3}d_1^2&gt;0\\), any little change away from \\(\\mathbf{x}^*\\) always makes \\(L\\) increase, the objective function really reaches local minimum at the point \\(\\mathbf{x}^*\\), which is also the global minimum point as well since there is no other local minimum. References [1] Wikipedia: Lagrange multiplier [2] Wikipedia: Karush–Kuhn–Tucker conditions [3] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix E. New York: Springer.","link":"/2018/04/21/KKT/"}],"tags":[{"name":"tech_memo","slug":"tech-memo","link":"/tags/tech-memo/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"math","slug":"math","link":"/tags/math/"}],"categories":[]}