{"pages":[],"posts":[{"title":"(PRML Notes) 1.1 Polynomial Curve Fitting as an Introductory Example","text":"A series of notes taken from Pattern Recognition and Machine Learning. This introduction chapter is mainly composed of these three theories: Probability theory provides a framework for expressing uncertainty in a precise and quantitative manner. Decision theory allows us to exploit probabilistic representation in order to make predictions that are optimal. Information theory studies the quantification, storage, and communication of information. Example: Polynomial Curve Fitting Consider a given data set \\(\\mathbf{x} \\equiv (x_1, ..., x_N)^T\\) and with the corresponding values \\(\\mathbf{t} \\equiv (t_1, ..., t_N)^T\\), where the values are generated by \\[ t_i = \\sin(2\\pi x_i) + \\epsilon, \\ i= 1,2,...,N \\] Here \\(\\epsilon\\) is a small random noise following a Gaussian distribution. A simple approach of curve fitting is to fit the data using a polynomial function \\[ y(x, \\mathbf{w}) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M = \\sum_{j=0}^M w_j x^j \\] It is not a linear function of \\(x\\), but linear in \\(\\mathbf{w}\\) by viewing \\(x^j\\) as the parameter of \\(w_j\\). The curve fitting can be done by minimizing an error function like \\[ E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 \\] Since the error function is quadratic of \\(\\mathbf{w}\\), the minimization of \\(E(\\mathbf{w})\\) has a unique solution \\(\\mathbf{x}^*\\). To compare different sizes of data sets on an equal footing, it is sometimes more convenient to use the root-mean-square(RMS) error \\[ E_{RMS} = \\sqrt{2E(\\mathbf{w}^*)/N} \\] However, minimizing the error is not the same thing as getting a better model. if our model is more complex, for example if \\(M\\) is too large, we may suffer from the over-fitting problem, namely our model will perform much worse on unseen data. Over-fitting problem. Generally speaking, I think the most data generated from nature follows the law of simplicity, something like the Occam's Razor, so one should select the model that makes the fewest assumptions. This is expressed as prior knowledge later in Bayesian probability later. One approach to avoid over-fitting is regularization, adding a penalty term to the error function, for example \\[ \\widetilde{E}(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\] Note that often the coefficient \\(w_0\\) is omitted from the regularization because its inclusion causes the results to depend on the choice of origin for the target variable. It may be included but with its own regularization coefficient. Perhaps under many circumstances, the data may be generated from some rule like \\(t_i = \\sin(2\\pi x_i) + b + \\epsilon\\), not simply as \\(t_i = \\sin(2\\pi x_i) + \\epsilon\\), and we should learn the natural bias instead of penalize it. Such techniques are known as shrinkage in statistics literature, the particular case of a quadratic regularizer is called ridge regression, and in the context of neural networks, this approach is known as weight decay. Selection of Models with different complexity To determine the suitable value of model complexity (either \\(M\\) or \\(\\lambda\\) here in the context), we can partition the available data into a training set, used to determine the coefficients \\(\\mathbf{w}\\), and a separate validation set, or called a hold-out set, used to optimize the model complexity.","link":"/2018/04/22/PRML1-1/"},{"title":"(PRML Notes) 1.3 Model Selection","text":"A series of notes taken from Pattern Recognition and Machine Learning. Training set is used to generate the predictive models and validation set to select the one with the best predictive performance. But if the model design is iterated many times using a limited size data set, then some over-fitting to the validation data can occur and we keep aside a third test set to evaluate the performance of the final selected model. Since separating a finite data set into these subsets is wasteful for data, we use K-fold cross-validation to assess performance. At the beginning the data is separated into groups, each time groups are used to train model and the remained one to evaluate. This procedure is repeated for all possible choices. When data is particularly scarce, we consider the case where the number of groups equals to the data points, which gives out the leave-one-out technique. 4-fold example. It's computationally expensive if \\(K\\) is large, and often we should evaluate a model under every possible hyperparameter choice, which require training runs exponential in the number of parameters. So we need to find a measure of performance which depends only on the training data and not suffers from bias due to over-fitting (so that only a single run is needed). Akaike information criterion (AIC) chooses model for which the quantity \\[ \\ln p(\\mathcal{D}|\\mathbf{w}_\\mathrm{ML}) -M \\] is largest, where \\(p(\\mathcal{D}|\\mathbf{w}_\\mathrm{ML})\\) is the best-fit likelihood, and \\(M\\) is the number of adjustable parameters in the model. Bayesian information criterion (BIC) is a variant of this quantity, will be discussed in section 4.4.1.","link":"/2018/04/28/PRML1-3/"},{"title":"(PRML Notes) 1.4 The Curse of Dimensionality","text":"A series of notes taken from Pattern Recognition and Machine Learning. Consider a naive \"cell\" solution for classification, which divides the input space into many cells and each test point is assigned to the class that has a majority number of representatives in the same cell. One problem is we need to gather exponentially large quantity of data as the number of cells grows exponentially in higher dimensions in order to ensure that the cells are not empty. Similarly, the number of coefficients in polynomial curve fitting will grow in power law as the dimension increases, making the method becomes unwieldy. For example an order-3 polynomial takes the form \\[ y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D\\sum_{j=1}^i w_{ij}x_ix_j + \\sum_{i=1}^D\\sum_{j=1}^i\\sum_{k=1}^j w_{ijk} x_i x_j x_k \\] On geometrical intuitions, the volume of \\(D\\)-dimensional spherecis \\(V_D(r)=K_Dr^D\\), where the constant \\(K_D\\) only depends on \\(D\\), and most of the volume is concentrated in a thin shell near the surface since \\[ \\frac{V_D(1) - V_D(1-\\epsilon)}{V_D(1)} = 1-(1-\\epsilon)^D \\] For a Gaussian as well, most of the probability mass is located within a thin shell at a specific radius. Plot of the probability density of a Gaussian w.r.t. \\(r\\). So not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions, such difficulty is called the curse of dimensionality. However, It's not a severe problem in real application because Real data is often confined to a subspace in high dimensional space (like high dimensional image data generated from \\(3D\\) real world) Real data typically exhibit some smoothness properties, where interpolation can be applied to gain more data (the same reason for regularization being applied)","link":"/2018/04/29/PRML1-4/"},{"title":"Planning by Dynamic Programming","text":"Dynamic programming assumes full knowledge of the MDP. Dynamic Programming Dynamic: sequential or temporal component to the problem (like a table storing subproblem solutions) Programming: optimizing a \"problem\", i.e. a policy DP is a very general solution method for problems which have two properties (MDP satisfies both): Optimal substructure (see the first theorem for optimal policy in last note) Overlapping subproblems (recurring many times, reused by Bellman equation) DP is used for planning in an MDP (recall that it means we have full knowledge of the MDP): For prediction Input: MDP \\(\\left\\langle \\mathcal{S,A,P,R,\\gamma}\\right\\rangle​\\) and policy \\(\\pi​\\) (or MRP \\(\\left\\langle \\mathcal{S,P,R,\\boldsymbol{\\gamma}}\\right\\rangle​\\) without any policy) Output: value function \\(v_{\\pi}\\) For control Input: MDP \\(\\left\\langle \\mathcal{S,A,P,R,\\gamma}\\right\\rangle\\) without policy Output: optimal value function \\(v_*\\) and optimal policy \\(\\pi_*\\) Policy Evaluation Problem: evaluate a given policy \\(\\pi\\). Solution: iterative application of Bellman expectation backup. (here backup means using future state value to update the past) By using synchronous backups: at each iteration \\(k+1\\), for all states \\(s\\in S\\), we update \\(v_{k+1}(s)\\) from \\(v_k(s&#39;)\\), where \\(s&#39;\\) is a successor state of \\(s\\). (\\(v_k(s)\\) means the value of state \\(s\\) at \\(k\\)th iteration.) Asynchronous backups will be discussed later (Convergence has been proven. And note that before convergence, in general, the values for all states do not correspond to any policy, so we denote the change of state value as \\(v_1 \\rightarrow v_2 \\rightarrow\\dots\\rightarrow v_\\pi\\).) We use the same Bellman expectation equation for all states at each iteration: \\[ v_{k+1}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_k(s&#39;) \\right) \\] Example: Small Gridworld \\(\\mathcal{S}\\) : nonterminal states \\(1,...,14\\) and one terminal state shown twice as shaded squares. \\(\\mathcal{A}​\\) : four directions. \\(\\mathcal{P}\\) : leading out of the grid leave state unchanged, otherwise with probability \\(1\\) move to the state directed. \\(\\mathcal{R}\\) : Reward is \\(−1\\) until the terminal state is reached. \\(\\mathcal{\\gamma}\\) : \\(1\\) Agent follows uniform random policy, which is going to be evaluated \\[ \\pi(n|\\cdot) = \\pi(w|\\cdot) = \\pi(s|\\cdot) = \\pi(e|\\cdot) = 0.25 \\] Initially, all state values are \\(0\\). At each iteration, we synchronously backup the value. In \\(k=2\\), we gain the value \\(-1.7\\) from \\(-1+0.25\\times ((-1)+(-1)+(-1)+0)\\). The value eventually converges to the true value under random policy, \\(v_\\pi\\). (The second column of the two pictures correspond to the policy improvement using current value.) Policy Iteration Policy iteration is used to improve a policy \\(\\pi\\), by following the two steps: Evaluate the policy \\(\\pi\\) (namely, policy evaluation) Improve the policy by acting greedily w.r.t. \\(v_\\pi\\) : \\[\\pi&#39;(s)=\\text{greedy}(v_\\pi(s)) = \\mathop{\\arg\\max}_{a\\in\\mathcal{A}} q_\\pi{(s,a)}\\] In the small gridworld example above, the improved policy after one evaluation pass was already optimal, while in general, it need more iterations of evaluation and improvement. It has been proven that policy iteration always converges to \\(\\pi^*\\). Example: Car Rental \\(\\mathcal{S}\\) : two locations, maximum \\(20\\) of each (\\(400\\) states in total). \\(\\mathcal{A}\\) : move up to 5 cars between locations overnight. \\(\\mathcal{P}\\) : cars are returned and requested randomly with prob \\(\\frac{\\lambda^n}{n!}e^{-\\lambda}\\) (Poisson distribution). 1st location: average requests = 3, average returns = 3 1st location: average requests = 4, average returns = 2 \\(\\mathcal{R}\\) : $10 for each car rented. \\(\\mathcal{\\gamma}\\) : \\(0.9\\) (There is a demonstration for policy improvement will always converge to an optimal policy in David Silver's course, but it seems that this demonstration does not figure out it won't be stuck in a local maximum. A formal version of the demonstration is by using contraction mapping.) Modified Policy Iteration Note that in small gridworld and many other problems, after only a few iteration will it be able to achieve optimal policy, so We can simply stop after \\(k\\) iterations Or introduce a stopping condition, e.g. \\(\\epsilon\\)-convergence of value function since there's no need to figure out the true value for every policy. A more radical but efficient way is to update policy after only one iteration. This is equivalent to value iteration. Generalized Policy Iteration As we will see later, policy evaluation may not strictly follow the Bellman Equation, neither does the policy improvement algorithm to be greedy. Value Iteration Principle of optimality: A policy \\(\\pi(a|s)\\) achieves the optimal value from state \\(s\\), \\(v_\\pi(s) = v_*(s)\\), if and only if for any state \\(s&#39;\\) reachable from \\(s\\), \\(\\pi\\) achieves the optimal value from \\(s&#39;\\), \\(v_\\pi(s) = v_*(s)\\). Following this principle, if we know the solution to subproblems \\(v_*(s&#39;)\\), then solution \\(v_*(s)\\) can be found by one-step lookahead (just the same idea in Bellman optimality equation) \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\right) \\] So the basic idea is starting from the final rewards and working backwards to update values iteratively. However, such idea also works with loopy, stochastic MDPs (those without final state), and finally updates all values to the optimal even their subproblems' solutions are unknown from the beginning. Example: Shortest Path Each step with reward -1 Here we iteratively applies Bellman optimality backup, and we get \\(v_1 \\rightarrow v_2 \\rightarrow \\cdots \\rightarrow v_*\\). At each iteration, we use synchronous backups. Note that unlike policy iteration, there is no explicit policy, intermediate value functions may not correspond to any policy. Summary of Synchronous DP Algorithms Problem Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration All the algorithms above are discussed based on state-value function, and the complexity is \\(O(mn^2)\\) per iteration, for \\(m\\) actions and \\(n\\) states. We could also apply the algorithms to action-value function, but the complexity will rise to \\(O(m^2n^2)\\). Reference [1] David Silver's RL course","link":"/2018/09/25/RL_DP/"},{"title":"Introduction to Reinforcement Learning","text":"RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future. About Reinforcement Learning Many Faces of Reinforcement Learning Reinforcement learning is about a science of decision making, sitting in many different fields of science. Branches of Machine Learning What makes RL different from other ML paradigms? No supervisor, only a reward signal Feedback is delayed, not instantaneous Time really matters (sequential, non i.i.d data) Agent’s actions affect subsequent data it receives The Reinforcement Learning Problem Reward Reward Hypothesis: All goals can be described by the maximization of expected reward. A reward \\(R_t\\) is a scalar feedback signal, indicating how well agent is doing at step \\(t\\). RL is based on the hypothesis showed above, aiming to taking actions to maximize the cumulative reward. Actions may have long term consequences Reward may delay Not greedy: better to sacrifice immediate reward to gain more long-term reward Agent and Environment Here the brain stands for the agent, the earth stands for the environment, showing the interaction: State The history is the sequence of observations, actions, rewards \\[ H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t \\] The state is the information to determine what happens next. Formally, state is a function of the history \\[ S_t = f(H_t) \\] The environment state \\(S_t^e\\) is the environment’s private representation, not usually visible to the agent, and even visible it may contain irrelevant information. The agent state \\(S_t^a\\) is the agent’s internal representation, used by RL algorithms, and can be any function of history \\(S_t^a = f(H_t)\\). Another independent state definition is information state (a.k.a. Markov state), which contains all useful information from the history. A state is Markov if and only if \\[ \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1,...,S_t] \\] Both the environment state \\(S_t^e\\) and \\(H_t\\) are Markov, but not always the agent state \\(S_t^a\\), which we need to design properly. Observability Fully observability: agent directly observes environment state, where \\(O_t=S_t^a=S_t^e\\). Formally, this is a Markov decision process (MDP). Partially observability: agent indirectly observes environment, now \\(S_t^a\\neq{S_t^e}\\). Formally, this is a partially observable Markov decision process (POMDP). When partially observable, agent must construct its own state representation \\(S_t^a\\), e.g. From complete history: \\(S_t^a=H_t\\) From beliefs of environment state: \\(S_t^a = (\\mathbb{P[S_t^e=s^1]},..,\\mathbb{P[S_t^e=s^n]})\\) From RNN: \\(S_{t}^{a} = \\sigma(S_{t-1}^aW_s+O_tW_o)\\) Inside RL Agent a maze example An RL agent may include: Policy: Agent’s Behavior It is a map from state to action, e.g. Deterministic policy: \\(a=\\pi(s)\\) Stochastic policy: \\(\\pi(a|s)=\\mathbb{P}[A_t=a|S_t=s]​\\) Arrows represents \\(\\pi(s)\\) at each state. Value Function: Prediction of Future Reward Numbers represents value \\(v_{\\pi}(s)\\) at each state. Used to select between actions, e.g. \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1}+\\gamma{R}_{t+2}+\\gamma^2{R}_{t+3}+...|S_t=s] \\] Model: Representation of Environment Grid layout represents transition model and numbers represent immediate reward from each state. Transitions: \\(\\mathcal{P}\\) predicts the next state e.g. \\[ \\mathcal{P}_{s\\rightarrow{s&#39;}}^a = \\mathbb{P}[S_{t+1}=s&#39;|A_t=a,S_t=s] \\] Rewards: \\(\\mathcal{R}\\) predicts the next (immediate) reward, e.g. \\[ \\mathcal{R}_s^a = \\mathbb{E}[R_{t+1}|A_t=a,S_t=s] \\] Categorizing RL agents Problems within RL Learning (RL) and Planning They are the two fundamental problems in sequential decision making. For learning, the environment is initially unknown, then the agent interacts with the environment and improves its policy. (e.g. new-born baby) For planning, a model of the environment is known, the agent performs computations with its model and improves its policy. (e.g. Go) Exploration and Exploitation Exploration finds more information about the environment. (e.g. try a new restaurant, possibly delicious or awful…) Exploitation exploits known information to maximize reward. (e.g. stay at home, just sleep~) Prediction and Control Prediction: given one policy, evaluate the future. Control: find the best policy to optimize the future, which need to evaluate all policies. Again here’s a grid world example, each step with reward -1, and if arrives at point A (B), you are immediately teleported to A’ (B’), with reward 10 (5). Prediction example: (b) shows the value function in each grid for the uniform random policy (randomly select a direction). Control example: (b) shows the optimal value function in each grid over all possible policies. Reference [1] David Silver's RL course","link":"/2018/09/18/RL_intro/"},{"title":"Hexo配置备忘","text":"Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Minos is a simple and retro styled Hexo theme, concentrating more on your ideas. 基本流程 hexo建站基本配置直接参照官网步骤，配置主题为Minos。 公式支持 网上广为流传的替换hexo默认渲染为hexo-renderer-kramed的方法，仍然会存在行内公式含两个引号时中间部分被渲染为斜体的问题。 最后参照Hexo 书写 LaTeX 公式时的一些问题及解决方法，替换默认渲染为hexo-renderer-pandoc。 -- 注意1：事先去pandoc官方下载最新版本的pandoc，否则直接使用apt install pandoc可能在run服务时因为版本过低出现pandoc: Unknown extension: smart的问题 -- 注意2：替换完成后主题内的依赖关系不会自动更改，需要将themes/minos/scripts/01_check.js中对默认渲染的依赖项'hexo-renderer-marked'手动替换为'hexo-renderer-pandoc' 图像居中 为了美观（强迫症），修改themes/source/css/style.scss中display属性为block，添加属性text-align: center。","link":"/2018/01/12/hexo-install/"},{"title":"Calculus of Variations","text":"Calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maximum and minimum of functionals. Introduction Functional is the function's function, in other words, functional takes a function as input and outputs a real number, or more mathematically, it refers to a mapping from a space \\(X\\) (commonly \\(X\\) is a space of functions) into the real numbers. In the calculus of variations, we seek a functional \\(f(x)\\) that maximizes (or minimizes) a functional \\(F[f]\\). For example, as we all know, the shortest curve between two fixed points is the straight line, and this can be proved using calculus of variations. Example 1: Shortest Curve To find out the shortest curve \\(y(x)\\) between two fixed points \\((a_1, b_1)\\) and \\((a_2, b_2)\\), we first express the length of the curve in the form of integration \\[ F[y] = \\int_{a_1}^{a_2} \\sqrt{1+[y&#39;(x)]^2} dx \\] where we have the boundary conditions of \\(y(x)\\) \\[ y(a_1) = b_1,\\ y(a_2) = b_2 \\] Suppose \\(y(x)\\) is twice continuously differentiable. Now if the functional \\(F\\) attains a local minimum at \\(y=f\\), consider \\(\\eta(x)\\) to be an arbitrary function that has at least one derivative and vanishes at end points \\(a_1\\) and \\(a_2\\), i.e. \\(\\eta(a_1) = 0\\) and \\(\\eta(a_2)=0\\), then we have \\[ F[f] \\leq F[f + \\epsilon\\eta] \\] where \\(\\epsilon\\) can be any number close to \\(0\\), here the term \\(\\epsilon \\eta\\) is called the variation of the function \\(f\\) and is denoted by \\(\\delta f\\). We can view \\(F[f+\\epsilon \\eta]\\) as a function of \\(\\epsilon\\) and denote it as \\(J(\\epsilon)\\), then \\(J&#39;(0) = 0\\) must be true for any arbitrary choice of \\(\\eta\\) since the functional \\(F[y]\\) achieves minimum when \\(y=f\\). By calculating \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\left. \\frac{d}{d\\epsilon}F[f+\\epsilon\\eta] \\right|_{\\epsilon=0} \\\\ &amp;= \\left. \\frac{d}{d\\epsilon} \\int_{a_1}^{a_2} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}dx \\right|_{\\epsilon=0} \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{d}{d\\epsilon} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{[f&#39;(x) + \\epsilon\\eta&#39;(x)]\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx \\end{aligned} \\] We get \\[ \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx = 0 \\] which means that in differentiable function space, \\(F[y]\\)'s derivatives in all directions (all \\(\\eta\\)) are zero. Since \\(y(x)\\) is twice continuously differentiable, we can further integrate the left hand side of above equation by parts \\[ \\begin{aligned} \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx &amp;= \\left. \\eta(x) \\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right|_{a_1}^{a_2} -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx \\\\ &amp;= -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx\\ \\ \\ \\text{(since $\\eta(a_1) = 0$ and $\\eta(a_2) = 0$)} \\end{aligned} \\] then now we have \\[ \\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx = 0 \\] Since \\(\\eta(x)\\) is arbitrary, imagine choosing a perturbation \\(\\eta(x)\\) that is zero everywhere except in the neighborhood of a point \\(\\hat{x}\\), in which case the functional derivative must be zero at \\(x=\\hat{x}\\), otherwise the integral will not be equal to zero. However, because this must be true for every choice of \\(\\hat{x}\\), the functional derivative must vanish for all values of \\(x\\), that is \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = 0 \\] This is called the fundamental lemma of calculus of variations. Follow this result we have \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = \\frac{f&#39;&#39;(x)}{(1 + [f&#39;(x)]^2)^{\\frac{3}{2}}} = 0 \\] So \\[ f&#39;&#39;(x)=0 \\] Euler–Lagrange Equation Now consider the more general case, for the functional \\[ F[y] = \\int_{x_1}^{x_2} L(x,y(x), y&#39;(x)) dx \\] Again suppose the functional attains a local minimum at \\(y=f\\), and define \\(J(\\epsilon) = F[f+\\epsilon\\eta]\\). Then we have \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\int_{x_1}^{x_2} \\left. \\frac{dL}{d\\epsilon} \\right|_{\\epsilon=0}dx \\\\ &amp;= \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f}\\eta +\\frac{\\partial L}{\\partial f&#39;}\\eta&#39; \\right) dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx +\\left.\\frac{\\partial L}{\\partial f&#39;}\\eta \\ \\right|_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\eta \\left(\\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\right) dx \\end{aligned} \\] According to the fundamental lemma of calculus of variations, we get \\[ \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] = 0 \\] which is called the Euler–Lagrange equation. The left hand side of this equation is called the functional derivative of \\(F[f]\\), denoted as \\[ \\frac{\\delta J}{\\delta f} = \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\] References [1] Wikipedia: Calculus of variations [2] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix D. New York: Springer.","link":"/2018/04/18/variations/"},{"title":"Lagrange Multiplier and KKT Conditions","text":"In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maximum and minimum of a function subject to equality constraints, while KKT conditions solve the problems with inequality constraints. Naive Approach Consider the problem of finding the maximum of a function \\(f(x_1, x_2)\\) subject to a constraint relating \\(x_1\\) and \\(x_2\\), which we write in the form \\[ g(x_1, x_2) = 0 \\] One approach would be to solve the above equation to express \\(x_2\\) as a function of \\(x_1\\) in the form of \\(x_2 = h(x_1)\\), then apply differentiation to \\(f(x_1, h(x_1))\\) w.r.t. \\(x_1\\) in the usual way. However, there are at least two drawbacks of this simple approach: Sometimes difficult or even impossible to find out an expression like \\(x_2 = h(x_1)\\) Spoil the natural symmetric between these variables A more elegant way is introducing a parameter \\(\\lambda\\) called a Lagrange multiplier. Lagrange Multiplier An Intuitive Understanding Now consider in a \\(D\\)-dimensional space where we need to \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = 0 \\] Geometrically, the constraint \\(g(\\mathbf{x}) = 0\\) represents a \\((D-1)\\)-dimensional surface, and what's worth noting is that at any point on the constraint surface, the gradient \\(\\nabla g(\\mathbf{x})\\) will be orthogonal to the surface. To see this, consider the total differential \\(dg = \\nabla g(\\mathbf{x})^T d \\mathbf{x}\\), then if the point \\(\\mathbf{x}+d\\mathbf{x}\\) also lies on the constraint surface, then \\(dg=0\\), so we have \\(\\nabla g(\\mathbf{x})^T d\\mathbf{x} = 0\\). Next, the point \\(\\mathbf{x}^*\\) on the constraint surface that maximize \\(f(\\mathbf{x})\\) must have the property that \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), in other words, \\(\\nabla g(\\mathbf{x}^*)\\) is orthogonal to the constraint surface, because otherwise we could increase the value of \\(f(\\mathbf{x})\\) by moving a short distance along the constraint surface. Personally, I think it's quite similar to the force resolution, where we can view the point \\(\\mathbf{x}\\) as a ball on its orbit -- the constraint surface. A force, \\(\\nabla f(\\mathbf{x})\\), is applied on it and the ball will finally moves up to a place where no force can be resolved on the tangent plane of the place. Since \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), there must exist a parameter \\(\\lambda\\) such that \\[ \\nabla f(\\mathbf{x}^*) +\\lambda \\nabla g(\\mathbf{x^*}) = 0 \\] where \\(\\lambda\\neq 0\\) is known as Lagrange multiplier. For representation convenience, we can introduce the Lagrangian function defined by \\[ L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) \\] By setting \\(\\nabla_{\\mathbf{x}} L=\\mathbf{0}\\) together with the constraint \\(g(\\mathbf{x}) = 0\\), we can get all the necessary conditions (but not sufficient) when \\(\\mathbf{x}\\) is an extreme point on the constraint surface. Example 1 \\[ \\max \\ 1-x_1^2 - x_2^2 \\\\ \\text{s.t.} \\ \\ x_1 + x_2 -1 =0 \\] The corresponding Lagrange function is given by \\(L(\\mathbf{x}, \\lambda) = 1-x_1^2 - x_2^2 + \\lambda (x_1 + x_2 -1)\\), then set \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_1} &amp;= -2x_1 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial x_2} &amp;= -2x_2 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial \\lambda} &amp;= x_1 + x_2 -1 = 0 \\end{aligned} \\] By solving the system of equations we get the stationary point on the constraint surface is \\((\\frac{1}{2},\\frac{1}{2})\\), and it can be verified that it is a maximum point by checking nearby points or using second-order sufficient conditions (see further reading 2). KKT Conditions So far we have considered the optimization problem with equality constraint only. Now consider such problem with inequality constraint \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) \\geq 0 \\] The solution of this problem can be classified into two kinds: When the constraint is active, that is when the stationary point lies on the boundary \\(g(\\mathbf{x})=0\\), the problem is just analogous to the one discussed previously and corresponds to a stationary point with \\(\\lambda \\neq 0\\). Note that now the sign of \\(\\lambda\\) is crucial, since \\(f(\\mathbf{x})\\) will only be at a maximum if \\(\\nabla f(\\mathbf{x})\\) is away from the region \\(g(\\mathbf{x})&gt;0\\). So further we have the corresponding \\(\\lambda &gt; 0\\). When the constraint is inactive, the stationary point lies in the region \\(g(\\mathbf{x})&gt;0\\), with corresponding \\(\\lambda = 0\\). Again for the convenience of representation, note that for both cases, the product \\(\\lambda g(\\mathbf{x})=0\\), thus the solution can be obtained by maximizing the Lagrange function w.r.t. \\(\\mathbf{x}\\) s.t. the conditions \\[ \\begin{aligned} g(\\mathbf{x}) &amp;\\geq 0 &amp; &amp; \\text{(constriant function)} \\\\ \\lambda &amp;\\geq 0&amp;&amp; \\text{(gradient direction)} \\\\ \\lambda g(\\mathbf{x})&amp;=0&amp;&amp; \\text{(complementary slackness condition)} \\end{aligned} \\] These are known as Karush-Kuhn-Tucker(KKT) conditions. If we need to apply KKT conditions to minimization problems, just change the sign of \\(\\lambda\\) in Lagrange function to keep the gradient direction reversed, i.e. \\(L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\\) ### Multiple Equality and Inequality Constraints Finally, it's now easy to extend the method to the case of multiple equality and inequality constraints \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\ \\ \\ \\ \\ \\ h_j(\\mathbf{x}) =0, \\ j=1,...,l \\] Define the Lagrange function to be \\[ L(\\mathbf{x},\\mathbf{v},\\mathbf{w}) = f(\\mathbf{x}) + \\sum_{i=1}^m w_i g_i(\\mathbf{x}) + \\sum_{j=1}^l v_j h_j(\\mathbf{x}) \\] Now the necessary conditions when a point \\(\\mathbf{x}\\) becomes the local optimum is \\[ \\left\\{\\begin{aligned} &amp; \\nabla_x L(\\mathbf{x},\\mathbf{w},\\mathbf{v}) = 0 \\\\&amp;g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\&amp;h_j(\\mathbf{x}) = 0, \\ j=1,...,l \\\\&amp; w_i \\geq 0,\\ i=1,...,m \\\\&amp; w_i g(\\mathbf{x})=0,\\ i=1,...,m \\end{aligned}\\right. \\] Example 2 \\[ \\min \\ x_1^2 - x_2 - 3x_3 \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = -x_1 - x_2 - x_3 \\geq 0 \\\\ \\ \\ \\ \\ \\ \\ h(\\mathbf{x}) = x_1^2 + 2x_2 - x_3 = 0 \\] Write down Lagrange function for minimization problem \\[ L(\\mathbf{x}, w, v) = (x_1^2 - x_2 - 3x_3) - w(-x_1 - x_2 - x_3) - v(x_1^2 + 2x_2 - x_3 ) \\] As described above, the first-order necessary condition of optimum is \\[ \\left\\{\\begin{aligned} &amp;L_{x_1}&#39; = 2x_1 + w - 2vx_1 =0 \\\\ &amp;L_{x_2}&#39; = -1 + w - 2v=0 \\\\ &amp;L_{x_3}&#39; = -3 + w +v=0 \\\\ &amp;-x_1 - x_2 - x_3 \\geq 0 \\\\ &amp; x_1^2 + 2x_2 - x_3 = 0 \\\\&amp;w\\geq 0 \\\\&amp; w(-x_1 - x_2 - x_3) = 0 \\end{aligned}\\right. \\] The solution is \\(\\mathbf{x}^* = (-\\frac{7}{2}, -\\frac{35}{12}, \\frac{77}{12}), \\ w = \\frac{7}{3}, \\ v=\\frac{2}{3}\\). Further Reading Lagrange Multipliers Can Fail To Determine Extrema In single variable case, when the gradient of the constraint function \\(\\nabla g(\\mathbf{x})=0\\), the simple Lagrangian multiplier discussed above doesn't work. e.g. \\(\\min \\ x \\text{, s.t.} \\ \\ g(x,y) = y^2 + x^4 - x^3=0\\). http://web.cs.iastate.edu/~cs577/handouts/lagrange-multiplier.pdf Explain Lagrange multiplier in multi-constraint case in detail, and give the second-order sufficient conditions, which use the definitive property of Lagrangian function's Hessian on the tangent space to verify if a stationary point is also an extreme point in a more mathematical way. Besides, it avoids discussing the cases like that mentioned in the above article by introducing the concept of regular point. Tangent space at x^* on a surface defined by (a) one constraint (shown as a tangent plane) and (b) two constraints (shown as a tangent line). According to the second-order sufficient conditions mentioned in further reading 2, we can verifying the solution as optimum in example 2 by first calculating the Hessian at this point \\[ \\nabla_{\\mathbf{x}}^2L(\\mathbf{x}^*, w, v) = \\begin{bmatrix} \\frac{2}{3} &amp;0&amp; 0 \\\\0&amp;0&amp;0\\\\0&amp;0&amp;0 \\end{bmatrix} \\] Since both the two constraints are active at \\(\\mathbf{x^*}\\), we can solving the systems of equations below to get the expression of the tangent space at this point \\[ \\left\\{ \\begin{aligned} \\nabla g(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\\\ \\nabla h(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\end{aligned} \\right. \\] where \\(\\mathbf{d} = (d_1, d_2, d_3)^T\\) stands for any feasible direction on the tangent space, and \\(g(\\mathbf{x}^*) = (-1,-1,-1)^T\\), \\(h(\\mathbf{x}^*) = (-7,2,-1)\\). By solving that we have \\(\\mathbf{d} = (d_1,2d_1,-3 d_1)^T\\). Because \\(\\mathbf{d}^T\\nabla^2_{\\mathbf{x}}L(\\mathbf{x}^*, w, v) \\mathbf{d}=\\frac{2}{3}d_1^2&gt;0\\), any little change away from \\(\\mathbf{x}^*\\) always makes \\(L\\) increase, the objective function really reaches local minimum at the point \\(\\mathbf{x}^*\\), which is also the global minimum point as well since there is no other local minimum. References [1] Wikipedia: Lagrange multiplier [2] Wikipedia: Karush–Kuhn–Tucker conditions [3] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix E. New York: Springer.","link":"/2018/04/21/KKT/"},{"title":"(PRML Notes) 1.2 Probability Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. Due to the finite size of data sets and the noise on measurements, we use probability to express the uncertainty in pattern recognition. Some concepts like sum rule, product rule, marginal probability, joint probability, conditional probability are very basic, but for this book it would be worthy of gaining insight into Bayes theorem, which plays a central role in PRML \\[ p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} \\] Using sum rule, the denominator, which can be viewed as a normalizer, can also be expressed in terms of quantities appearing in the numerator \\[ p(X) = \\sum_Y p(X|Y)p(Y) \\] Probability Densities If the probability of a real-valued variable \\(x\\) falling in the interval \\((x, x+\\delta x)\\) is given by \\(p(x)\\delta x\\) for \\(\\delta x \\rightarrow 0\\), then \\(p(x)\\) is called the probability density over \\(x\\). If \\(x\\) is a discrete variable, then \\(p(x)\\) is sometimes called a probability mass function. The probability density \\(p(x)\\) must satisfy two conditions \\[ \\begin{align} p(x) &amp;\\geq 0 \\\\ \\int_{-\\infty}^\\infty p(x) \\mathrm{d}x &amp;= 1 \\end{align} \\] One point worth noting is that under a nonlinear change of variable, a probability density transforms differently from a simple function replacement, but due to a Jacobian factor. Consider a change \\(x=g(y)\\), and \\(p_x(x)\\) and \\(p_y(y)\\) are different densities corresponding to each variables, since for some small values of \\(\\delta x\\), the observed values falling in the range \\((x, x+\\delta x)\\) will be transformed into the range \\((y, y + \\delta y)\\), or sometimes \\((y - \\delta y, y)\\), we have \\[ p_x(x)|\\delta x| \\simeq p_y(y) |\\delta y| \\] and hence \\[ \\begin{align} p_y(y) &amp;= p_x(x)\\left| \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\right| \\\\ &amp;= p_x(g(y))|g&#39;(y)| \\end{align} \\] Intuitively, the larger the derivative \\(g&#39;(y)\\) is, the sparser the changed distribution \\(p_x(x)\\) is. One consequence is that the maximum of a probability density is dependent on the choice of variable. Here \\(y = g^{-1}(x) =1/(1+exp(-x+5))\\) is a logistic sigmoid function, and \\(p_x(x)\\) is a Gaussian \\(\\mathcal{N}(6, 1)\\). The green curve stands for \\(p_x(g(y))\\), while the purple one stands for \\(p_x(g(y))|g&#39;(y)|\\), with a different maximum. Expectations and Covariances Most are mentioned in statistics class. Here a subscript is used when we consider expectations of functions of several variables to indicate which variable is being averaged over, e.g. below we only average over \\(x\\) \\[ \\mathbb{E}_x[f(x,y)] = \\sum_x p(x)f(x,y) \\] But personally I think this definition requires that \\(y\\) is irrelevant to \\(x\\), or just a parameter, otherwise it's somewhat nonsense if we multiply \\(f(x,y)\\), which considers a specific \\(y\\), with a marginal distribution \\(p(x)\\), which already integrated out the \\(y\\). Sometimes we also consider a conditional expectation, e.g. now we average \\(x\\) with \\(y\\) fixed \\[ \\mathbb{E}_x[f(x,y)|y] = \\sum_x p(x|y)f(x,y) \\] In the case of two random variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the covariance is a matrix \\[ \\begin{align} \\mathrm{cov} [\\mathbf{x,y}] &amp;= \\mathbb{E}_\\mathbf{x,y}[\\{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T - \\mathbb{E}[\\mathbf{y^T}]\\}] \\\\ &amp;= \\mathbb{E}_\\mathbf{x,y}[\\mathbf{xy}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^T] \\end{align} \\] If we consider the covariance of the components of \\(\\mathbf{x}\\) with each over, we denote \\(\\mathrm{cov}[\\mathbf{x}] \\equiv \\mathrm{cov}[\\mathbf{x},\\mathbf{x}]\\). Bayesian Probabilities Frequentists view probability as frequencies of random events, while Bayesian view probability as quantification of uncertainty. Given a data set \\(\\mathcal{D} = \\{t_1,...,t_n\\}\\) and model parameters \\(\\mathbf{w}\\), Bayes theorem takes the form \\[ p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})} \\] allowing us to evaluate uncertainty after \\(\\mathcal{D}\\) is observed. Here \\(p(\\mathbf{w}|\\mathcal{D})\\) is called the posterior probability, \\(p(\\mathcal{D}|\\mathbf{w})\\) can be viewed as a function of \\(\\mathbf{w}\\) called likelihood function, and \\(p(\\mathbf{w})\\) is prior probability where the prior knowledge is included. We can state the theorem in the form \\[ \\mathrm{posterior \\propto likelihood \\times prior} \\] By integrating both sides of the equation above, we have \\[ {p(\\mathcal{D})} = \\int p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})\\ \\mathrm{d}\\mathbf{w} \\] The likelihood function plays a central role in both Bayesian and frequentist paradigms. In a frequentist setting, \\(\\mathbf{w}\\) is being fixed, usually determined by some estimator like maximum likelihood, which is the same as minimizing the negative logarithm of it called error function. For example, we will later see that minimizing \\(E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2\\) is a form of likelihood maximization. While from the Bayesian viewpoint, there is only a single \\(\\mathcal{D}\\) and uncertainty is expressed through \\(p(\\mathbf{w})\\). Bootstrap is a frequentist way to evaluate accuracy, for example given a set \\(\\mathcal{D} = \\{1,2,3,4,5\\}\\), multi-datasets are generated like \\(\\{4,3,4,1,5\\}\\), \\(\\{1,3,5,5,1\\}\\), which can contain duplicates and evaluation is done on these new datasets. The Gaussian Distribution Univariate Gaussian distribution is defined by \\[ \\mathcal{N} (x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\mathrm{exp}\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\] Note that the reciprocal of the variance in this book is often written as \\(\\beta = 1/\\sigma^2\\) called the precision. Multivariate \\(D\\)-dimensional Gaussian distribution is defined by \\[ \\mathcal{N} (\\mathbf{x}|\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2} |\\mathbf{\\Sigma}|^{1/2}} \\mathrm{exp}\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\} \\] Suppose a dataset \\(\\mathtt{x} = (x_1,...,x_N)^T\\) sampled from a univariate Gaussian, i.i.d. between each of them (independently and identically distributed). So the probability of the data set given \\(\\mu\\) and \\(\\sigma^2\\) is \\[ p(\\mathtt x| \\mu , \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n| \\mu, \\sigma^2) \\] One way to determine the parameters is to maximize the likelihood function above, which can be changed into the logarithm form \\[ \\ln p(\\mathtt x| \\mu , \\sigma^2) = -\\frac{N}{2}\\ln (2\\pi) - N \\ln \\sigma - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 \\] By first differentiating it w.r.t. \\(\\mu\\) we get the solution of \\(\\mu\\) is just the sample mean \\[ \\mu_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N x_n \\] and then differentiating it w.r.t. \\(\\sigma^2\\) and use the solution of \\(\\mu\\) above we get the solution of \\(\\sigma^2\\) is just the sample variance \\[ \\sigma^2_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu_\\mathrm{ML})^2 \\] However, in fact, maximum likelihood approach underestimates the variance of the distribution \\[ \\mathbb{E} [\\sigma^2_\\mathrm{ML}] = \\frac{N-1}{N} \\sigma^2 \\] which can be easily proved by using \\(\\mathbb{E}[x_nx_m] = \\mu^2 + I_{nm}\\sigma^2\\), where \\(I_{nm}=1\\) if \\(n=m\\) and \\(I_{nm}=1\\) otherwise. Although the influence of underestimation will be less severe as \\(N\\) grows larger, however, more complex models with many parameters will still suffer. Bias arises in using maximum likelihood to determine the variance of a Gaussian. Curve Fitting Re-visited Previously we solved this problem by minimizing error, now from a probabilistic perspective we can gain some insights into error function, regularization and then a full Bayesian treatment. Now we first use a Gaussian to predict the distribution of data \\[ p(t|x,\\mathbf{w}, \\beta) = \\mathcal{N} (t| y(x,\\mathbf{w}), \\beta^{-1}) \\] Here \\(y(x,\\mathbf{w})= \\sum_{j=0}^M w_j x^j\\) is the polynomial function defined before, and \\(\\beta = 1/\\sigma^2\\) is called the precision of the Gaussian. Using maximum likelihood on the training data \\(\\{\\mathtt{x}, \\mathtt{t}\\}\\), the likelihood function is given by \\[ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) = \\prod_{n=1}^N\\mathcal{N} (t_n| y(x_n,\\mathbf{w}), \\beta^{-1}) \\] And we have \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\frac{1}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 \\end{aligned} \\] We can see that maximizing the log likelihood reach the same optimum as minimizing the sum-of squares error function. Further maximize w.r.t. \\(\\beta\\) we get \\[ \\frac{1}{\\beta_\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^N [y(x_n,\\mathbf{w}_\\mathrm{ML})-t_n]^2 \\] Now we have a probabilistic model expressed in terms of the predictive distribution by \\[ p(t|x,\\mathbf{w}_\\mathrm{ML}, \\beta_\\mathrm{ML}) = \\mathcal{N} (t| y(x,\\mathbf{w}_\\mathrm{ML}), \\beta_\\mathrm{ML}^{-1}) \\] For a more Bayesian approach, we introduce a prior distribution over \\(\\mathbf{w}\\) as Gaussian for simplicity \\[ p(\\mathbf{w}|\\alpha) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) = \\left(\\frac{\\alpha}{2\\pi}\\right)^{(M+1)/2} \\exp \\left\\{-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right\\} \\] Using Bayes theorem, \\[ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) = \\frac{p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)}{p(\\mathtt{t}|\\mathtt{x}, \\beta)} \\propto p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha) \\] To maximize the posterior probability w.r.t. \\(\\mathbf{w}\\), it is equivalent to \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln [p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)] \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}+ \\frac{M+1}{2}\\ln\\alpha - \\frac{M+1}{2}\\ln(2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w} \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 +\\frac{\\alpha}{\\beta}\\mathbf{w}^T\\mathbf{w} \\end{aligned} \\] This technique is called maximum posterior (MAP), and we can see that maximizing the posterior is equivalent to minimizing the regularized sum-of-square error with \\(\\lambda = \\alpha/\\beta\\). Bayesian Curve Fitting So far the predictive distribution gives a non-point estimate for \\(t\\), but it is predicted under a point estimate \\(\\mathbf{w}_\\mathrm{ML}\\), or \\(\\mathbf{w}_\\mathrm{MAP}\\). In a fully Bayesian approach, we should integrate all values of \\(\\mathbf{w}\\) \\[ p(t|x,\\mathtt{x},\\mathtt{t}) = \\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathtt{x},\\mathtt{t})\\mathrm{d}\\mathbf{w} \\] Here the precisions \\(\\alpha\\) and \\(\\beta\\) are assumed fixed and known in advance, which will discussed in section 3.3, together with the solution of the integral above.","link":"/2018/04/23/PRML1-2/"},{"title":"(PRML Notes) 1.6 Information Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. When observing a specific value of \\(x\\), the amount of information received can be viewed as the \"degree of surprise\". Higher information is received when a highly improbable value of \\(x\\) has just observed, so the measure of information will depend monotonically on \\(p(x)\\), we denote it as \\(h(x)\\). Entropy of Discrete Distribution For independent variables \\(x\\) and \\(y\\), for which \\(p(x,y) = p(x)p(y)\\), the information should be addictive, i.e. \\(h(x,y) = h(x) + h(y)\\), meaning that the information gained from observing both of them should be the sum of the information gained from each of them separately. Then it can be shown that the relationship between \\(h(x)\\) and \\(p(x)\\) must be logarithm, so we can define \\[ h(x) = -\\log_2p(x) \\] The basis is chosen to be 2 by convention, so that the units of \\(h(x)\\) can be interpreted as bits used when transmitting a specific value of \\(x\\). Following this interpretation, the average amount of bits during one transmission is \\[ \\mathrm{H}[x] = - \\sum_x p(x)\\log_2p(x) \\] Image of \\(y=x\\log_2 x\\), the maximum locates at \\((\\frac{1}{e}, \\frac{1}{e\\ln 2})\\). This important quantity is called the entropy of the random variable \\(x\\). As the noiseless coding theorem stated, the entropy is a lower bound on the number of bits needed to transmit the state of a random variable. For example, Consider a sender wishes to transmit a variable \\(x\\) having 8 possible states \\(\\{a,b,c,d,e,f,g,h\\}\\), for which the respective probabilities are given by \\(\\left( \\frac{1}{2}, \\frac{1}{4},\\frac{1}{8},\\frac{1}{16},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64}\\right)\\), according to Haffman coding the optimal set of coding string is \\(0\\), \\(10\\), \\(110\\), \\(1110\\), \\(111100\\), \\(111101\\), \\(111110\\), \\(111111\\), hence the average length of code to be transmitted is \\[ \\frac{1}{2} \\times 1 +\\frac{1}{4} \\times 2 +\\frac{1}{8} \\times 3 +\\frac{1}{16} \\times 4 +4 \\times \\frac{1}{64} \\times 6 = 2\\ \\mathrm{bits} \\] which is the same as the entropy of this random variable \\[ \\mathrm{H}[x] = -\\frac{1}{2} \\times \\log_2 \\frac{1}{2} -\\frac{1}{4} \\times \\log_2 \\frac{1}{4} -\\frac{1}{8} \\times \\log_2 \\frac{1}{8} -\\frac{1}{16} \\times \\log_2 \\frac{1}{16} -4\\times\\frac{1}{64} \\times \\log_2 \\frac{1}{64} = 2 \\ \\mathrm{bits} \\] Note that the nonuniform distribution has a smaller entropy than a uniform one, which can be interpreted in terms of disorder. Consider sending another variable having 8 possible states but with equal possibilities, now the entropy is given by \\[ \\mathrm{H}[x] = -8 \\times \\frac{1}{8} \\times \\log_2 \\frac{1}{8} = 3 \\ \\mathrm{bits} \\] More generally, we switch to the use of natural logarithm in entropy in order to follow its much earlier origins in physics, and this will also provide a more convenient link with ideas in later chapters. To understand this view of entropy, consider a set of \\(N\\) identical objects that are to be divided amongst \\(M\\) bins, so that there are \\(n_i\\) objects in the \\(i^\\mathrm{th}\\) bin. The total number of ways allocating, which is also called the multiplicity, is \\[ W = C_N^{n_1}C_{N-n_1}^{n_2}C_{N-n_1-n_2}^{n_3}\\cdots C_{N-n_1-\\cdots-n_{M-1}}^{n_M} = \\frac{N!}{\\prod_in_i!} \\] In physics, the entropy is defined as the logarithm of the multiplicity scaled by an appropriate constant \\[ \\mathrm{H} = \\frac{1}{N}\\ln W = \\frac{1}{N}\\ln N! -\\frac{1}{N}\\sum_i \\ln n_i! \\] Now consider the limit \\(N \\rightarrow \\infty\\) with \\(n_i/N\\) held fixed, applying Stirling approximation, \\(N! \\approx \\sqrt{2\\pi N} (N/e)^N\\), we have \\[ \\ln N! \\approx N\\ln N - N \\] where the \\(O(\\ln N)\\) term is dropped. This gives \\[ \\begin{aligned} \\mathrm{H} &amp;= \\lim_{N\\rightarrow\\infty} \\ln N -1 -\\frac{1}{N} \\sum_i (n_i \\ln n_i -n_i) \\\\ &amp;= \\lim_{N\\rightarrow\\infty}\\ln N -\\frac{1}{N} \\sum_i (n_i \\ln n_i) \\\\ &amp;= \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_i n_i\\ln N -\\frac{1}{N} \\sum_i (n_i \\ln n_i) \\\\ &amp;= -\\lim_{N\\rightarrow\\infty}\\sum_i \\frac{n_i}{N} \\ln \\frac{n_i}{N} \\\\ &amp;= -\\sum_i p_i \\ln p_i \\end{aligned} \\] Here \\(p_i = \\lim_{N\\rightarrow\\infty}\\frac{n_i}{N}\\) is the probability of an object being assigned to the \\(i^\\mathrm{th}\\) bin. We can interpret the bins as the states \\(x_i\\) of a discrete random variable \\(X\\), where \\(p(X=x_i) = p_i\\), now the entropy is given in the similar form as before \\[ \\mathrm{H}[p] =-\\sum_i p(x_i) \\ln p(x_i) \\] The maximum of the entropy above can be obtained by using Jensen's inequality \\[ \\begin{aligned} \\mathrm{H}[p] &amp;=-\\sum_i p(x_i) \\ln p(x_i) \\\\ &amp;= \\sum_i p(x_i) \\ln \\frac{1}{p(x_i)} \\\\ &amp;\\leq \\ln \\sum_i p(x_i) \\frac{1}{p(x_i)} \\\\ &amp;= \\ln M \\end{aligned} \\] Here \\(M\\) is the total number of states, or bins. The maximum is achieved if and only if \\(p(x_1) = p(x_2)=p(x_M)=1/M\\). Intuitively speaking, the more scatter the distribution is, the larger the entropy is. Entropy of Continuous Distribution The definition of entropy can be extended to include distributions over continuous variables \\(x\\) as follows. First divide \\(x\\) into bins of width \\(\\Delta\\), then according to mean value theorem, in the \\(i^\\mathrm{th}\\) bin there must exist a value \\(x_i\\) such that \\[ \\int_{i\\Delta}^{(i+1)\\Delta} p(x) \\mathrm{d}x = p(x_i)\\Delta \\] Here \\(p(x_i)\\Delta\\) is exactly the probability of \\(x\\) allocated to the \\(i^\\mathrm{th}\\) bin. So now the entropy takes the form \\[ \\begin{aligned} \\mathrm{H}_\\Delta &amp;= - \\sum_i p(x_i)\\Delta \\ln (p(x_i)\\Delta) \\\\ &amp;= - \\sum_i p(x_i)\\Delta \\ln p(x_i)- \\sum_i p(x_i)\\Delta \\ln \\Delta \\\\ &amp;= - \\sum_i p(x_i)\\Delta \\ln p(x_i)- \\ln \\Delta \\end{aligned} \\] If we omit the second term \\(-\\ln\\Delta\\) on r.h.s. and consider the limit \\(\\Delta \\rightarrow 0\\), we have \\[ \\mathrm{H}_\\Delta = - \\int p(x) \\ln p(x) \\mathrm{d}x \\] This is called the differential entropy. The omitted term \\(\\Delta \\rightarrow 0\\) reflects the fact that to specify a continuous variable very precisely requires a large number of bits, and it is irrelevant to \\(p(x)\\), that is why we should omit it. The maximum of the entropy above can be obtained under the three constraints \\[ \\begin{aligned} \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x\\ &amp;=\\ 1 \\\\ \\int_{-\\infty}^{\\infty} xp(x) \\mathrm{d}x\\ &amp;=\\ \\mu \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^2p(x) \\mathrm{d}x\\ &amp;=\\ \\sigma^2 \\end{aligned} \\] Using Lagrange multiplier we get \\[ -\\int_{-\\infty}^{\\infty} p(x)\\ln p(x) \\mathrm{d}x+ \\lambda_1 \\left(\\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x-1\\right) + \\lambda_2\\left(\\int_{-\\infty}^{\\infty} xp(x) \\mathrm{d}x - \\mu \\right) + \\lambda_3 \\left(\\int_{-\\infty}^{\\infty} (x-\\mu)^2p(x) \\mathrm{d}x- \\sigma^2\\right) \\] and then maximize the functional w.r.t. \\(p(x)\\) by setting the derivative of this functional to zero, giving \\[ -1 - \\ln p(x) +\\lambda_1 +\\lambda_2 x+ \\lambda_3 (x-\\mu)^2 = 0 \\] Then \\[ p(x) = \\exp \\left\\{ -1 +\\lambda_1 +\\lambda_2 x+ \\lambda_3 (x-\\mu)^2 \\right\\} \\] Solving the last equation together with three constraints (this is somewhat difficult, I don't knowhow to solve actually...) we get \\[ \\begin{aligned} \\lambda_1\\ &amp;=\\ 1-\\frac{1}{2} \\ln (2\\pi \\sigma^2) \\\\ \\lambda_2 \\ &amp;=\\ 0 \\\\ \\lambda_3\\ &amp;=\\ -\\frac{1}{2\\sigma^2} \\end{aligned} \\] We see that \\(p(x)\\) is actually Gaussian \\[ p(x) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\} \\] This is why Gaussian is found widely in nature, which is developing with the trend of entropy increase. We can see that the differential entropy, unlike the discrete entropy, can be negative. For example, the entropy of Gaussian is \\[ \\begin{aligned} \\mathrm{H}[x] &amp;= - \\int_{-\\infty}^\\infty p(x) \\ln p(x) \\mathrm{d}x \\\\ &amp;= - \\int_{-\\infty}^\\infty p(x) \\left( -\\frac{1}{2}\\ln (2\\pi\\sigma^2) -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\\mathrm{d}x \\\\ &amp;= \\frac{1}{2}\\ln (2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\int_{-\\infty}^\\infty p(x) (x-\\mu)^2 \\mathrm{d}x \\\\ &amp;= \\frac{1}{2}\\ln (2\\pi\\sigma^2) +\\frac{1}{2} \\end{aligned} \\] \\(\\mathrm{H}[x]&lt;0\\) for \\(\\sigma^2 &lt; 1/(2\\pi e)\\). We omitted the term \\(-\\ln \\Delta\\) in original entropy expression before, which is an infinite positive term. Conditional Entropy Now consider a joint distribution \\(p(x,y)\\), if a value of \\(x\\) is already known, then the additional information needed to specify the corresponding value of \\(y\\) is given by \\(-\\ln p(y|x)\\), thus the average additional information needed to specify \\(y\\) is \\[ \\mathrm{H}[y|x] = -\\int\\int p(y,x)\\ln p(y|x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\] which is called the conditional entropy of \\(y\\) given \\(x\\). We can see that \\[ \\begin{aligned} \\mathrm{H}[y,x] &amp;= -\\int\\int p(y,x)\\ln p(y,x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\\\ &amp;= -\\int\\int p(y,x)\\ln p(y|x)\\ \\mathrm{d}y\\ \\mathrm{d}x\\ -\\int\\int p(y,x)\\ln p(x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\\\ &amp;= \\mathrm{H}[y|x] + \\mathrm{H}[x] \\end{aligned} \\] which means the information needed to describe \\(x\\) and \\(y\\) is given by the sum of the information needed to describe \\(x\\) alone plus the additional information required to specify \\(y\\) given \\(x\\). KL Divergence Now it's time to relate these idea to pattern recognition. Given an unknown distribution \\(p(x)\\), which has been approximated using distribution \\(q(x)\\), the KL divergence, or the relative entropy is the average addition amount of information required to transmit values of \\(x\\) if we use \\(q(x)\\) instead of \\(p(x)\\) to construct a coding scheme. So under this definition we have \\[ \\begin{aligned} \\mathrm{KL}(p|| q) &amp;=- \\int p(x) \\ln q(x) \\mathrm{d}x - \\left(- \\int p(x) \\ln p(x) \\mathrm{d}x \\right)\\\\\\\\ &amp;=- \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\\\ \\end{aligned} \\] Note that \\(\\mathrm{KL}(p|| q) \\neq \\mathrm{KL}(q|| p)\\). And since \\(-\\ln x\\) is strictly concave, using Jensen inequality we have \\[ \\mathrm{KL}(p|| q) = - \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\geq - \\ln\\int p(x) \\left( \\frac{q(x)}{p(x)} \\right)\\mathrm{d}x = -\\ln 1 = 0 \\] which shows that \\(\\mathrm{KL}(p|| q) \\geq 0\\) with equality if and only if \\(p(x)= q(x)\\). Thus we can interpret the KL divergence as a measure of the dissimilarity of the two distributions, and we can show that minimizing the KL divergence is equivalent to maximizing the likelihood function. Suppose we try to approximate \\(p(x)\\) using a parametric distribution \\(q(x|\\theta)\\). Although we don't know \\(p(x)\\) to compute KL divergence directly, we can sample from \\(p(x)\\), \\[ \\begin{aligned} \\mathrm{KL}(p|| q) &amp;= - \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\\\ &amp;\\simeq - \\frac{1}{N} \\sum_n \\ln \\left\\{ \\frac{q(x_n|\\theta)}{p(x_n)} \\right\\} \\\\ &amp;= - \\frac{1}{N} \\sum_n \\ln q(x_n|\\theta) + \\frac{1}{N} \\sum_n \\ln p(x_n) \\end{aligned} \\] Then \\[ \\mathop{\\arg \\min}_\\theta \\mathrm{KL}(p|| q) = \\mathop{\\arg \\max}_\\theta \\sum_n \\ln q(x_n|\\theta) \\] Mutual Information If the two variables of distribution \\(p(x,y)​\\) are not independent, we can measure how \"close\" they are to being independent by using KL divergence \\[ \\begin{aligned} \\mathrm{I}[x,y] &amp;= \\mathrm{KL}(p(x,y)||p(x)p(y))\\\\ &amp;= - \\int\\int p(x,y) \\ln \\left\\{ \\frac{p(x)p(y)}{p(x,y)} \\right\\} \\ \\mathrm{d}x\\ \\mathrm{d}y \\end{aligned} \\] Here \\(\\mathrm{I}[x,y]\\) is called the mutual information between the variables \\(x\\) and \\(y\\). We can see that \\(\\mathrm{I}[x,y] \\geq 0\\) with equality iff \\(x\\) and \\(y\\) are independent. Note that the fraction in logarithm can be rewritten as \\(p(x)/p(x|y)\\) or \\(p(y)/p(y|x)\\), the mutual information can be related to the conditional entropy through \\[ \\mathrm{I}[x,y] = \\mathrm{H}[x] - \\mathrm{H}[x|y] = \\mathrm{H}[y] - \\mathrm{H}[y|x] \\] From this point of view, the mutual information represents the reduction in uncertainty about \\(x\\) as a consequence of the new observation \\(y\\), or vice versa.","link":"/2018/12/24/PRML1-6/"},{"title":"(PRML Notes) 1.5 Decision Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. Decision theory allows us to make optimal decisions in situations involving uncertainty. Informally, for classification problem, we are interested in the posterior probability given data which belongs to a class \\(\\mathcal{C}_k\\) \\[ p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\mathbf{x})} \\] Our aim is to minimize the chance of assigning \\(\\mathbf{x}\\) to the wrong class, then intuitively we would choose the class having the higher posterior probability. Actually the intuition is correct, which is shown below. Minimizing the Miss Classification Rate Suppose the input space is divided into regions \\(\\mathcal{R_k}\\), called decision regions, one for each class, so that all points in it are classified to \\(\\mathcal{C_k}\\). The boundaries between decision regions are called decision boundaries or decision surfaces. First consider the binary classification, the probability of a mistake that \\(\\mathbf{x}\\) in \\(\\mathcal{C_1}\\) assigned to \\(\\mathcal{C_2}\\) and vice versa is \\[ \\begin{aligned} p(\\mathrm{mistake}) &amp;= p(\\mathbf{x}\\in \\mathcal{R}_1, \\mathcal{C_2}) + p(\\mathbf{x}\\in \\mathcal{R}_2, \\mathcal{C_1}) \\\\&amp;= \\int_\\mathcal{R_1} p(\\mathbf{x}, \\mathcal{C}_2)\\ \\mathrm{d}\\mathbf{x} + \\int_\\mathcal{R_2} p(\\mathbf{x}, \\mathcal{C}_1)\\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] For a given value of \\(\\mathbf{x}\\), to minimize \\(p(\\mathrm{mistake})\\), obviously we should assign it to class \\(\\mathcal{C_1}\\) if \\(p(\\mathbf{x}, \\mathcal{C}_1) &gt; p(\\mathbf{x}, \\mathcal{C}_2)\\). And since \\(p(\\mathbf{x}, \\mathcal{C}_k) = p(\\mathcal{C}_k|\\mathbf{x})p(\\mathbf{x})\\), where \\(p(\\mathbf{x})\\) is common for both terms, we can only compare the posterior \\(p(\\mathcal{C}_k|\\mathbf{x})\\), and the minimum \\(p(\\mathrm{mistake})\\) is achieved by assigning \\(\\mathbf{x}\\) to the class with the larger \\(p(\\mathcal{C}_k|\\mathbf{x})\\). The decision boundary is at \\(x_0\\), since when \\(\\hat{x}\\) moves towards \\(x_0\\), the red error area disappears. For the more general case of \\(K\\) classes, the mistake occurs when \\(\\mathbf{x}\\) in class \\(\\mathcal{C_k}\\) is assigned to \\(\\mathcal{C_j}\\) \\[ p(\\mathrm{mistake}) = \\sum_j \\sum_{k\\neq j}\\int_\\mathcal{R_j} p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] It is equivalent and more clear to maximize the probability of being correct \\[ p(\\mathrm{correct}) = 1-p(\\mathrm{mistake}) =\\sum_k \\int_\\mathcal{R_k} p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] Again if aim to minimizing the misclassification rate only, each single \\(\\mathbf{x}\\) should be assigned to the class having the maximum posterior \\(p(\\mathcal{C}_k|\\mathbf{x})\\). Minimizing the Expected Loss Usually some misclassification like diagnosing a patient with cancer as healthy is much more severe than the opposite case and should be punished more. Here we can introduce a loss matrix like \\[ \\begin{array}{@{}r@{}c@{}c@{}c@{}c@{}l@{}} &amp; \\mathrm{cancer} &amp; \\mathrm{normal} \\\\ \\left.\\begin{array} {c} \\mathrm{cancer} \\\\ \\mathrm{normal} \\end{array}\\right( &amp; \\begin{array}{c} 0 \\\\ 1 \\end{array} &amp; \\begin{array}{c} 1000 \\\\ 0 \\end{array} &amp; \\left)\\begin{array}{c} \\\\ \\\\ \\end{array}\\right. \\end{array} \\] Here 1000 denotes a man with cancer diagnosed as normal will be punished 1000 times than a normal man misdiagnosed. Again the mistake occurs when \\(\\mathbf{x}\\) in class \\(\\mathcal{C_k}\\) is assigned to \\(\\mathcal{C_j}\\), and the expected loss is given by \\[ \\mathbb{E}[L]= \\sum_j \\sum_{k}\\int_\\mathcal{R_j} L_{kj}p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] Consider for a single data \\(\\mathbf{x}\\) and choose the region \\(\\mathcal{R_j}\\) it belongs to, we should minimize \\(\\sum_k L_{kj}p(\\mathbf{x}, \\mathcal{C}_k)\\), and as before it is equivalent to minimize \\(\\sum_k L_{kj}p( \\mathcal{C}_k|\\mathbf{x})\\) w.r.t. \\(j\\). For example, if a patient with diagnosis \\(\\mathbf{x}\\) has \\(0.1\\) probability having cancer, i.e. \\(p( \\mathcal{C}_\\mathrm{cancer}|\\mathbf{x})=0.1\\) and \\(p( \\mathcal{C}_\\mathrm{normal}|\\mathbf{x})=0.9\\), then the expected error at this specific point \\(\\mathbf{x}\\) is \\(0*0.1+1*0.9 = 0.9\\) if we classify it as having cancer and \\(1000*0.1+0*0.9=100\\) if we classify it as normal. The Rejection Opinion Usually it will be appropriate to avoid making decision if we are uncertain about class membership, by introducing a threshold \\(\\theta\\) to reject those inputs \\(\\mathbf{x}\\) where \\(\\max_k p( \\mathcal{C}_k|\\mathbf{x}) &lt; \\theta\\). Inference and Decision Now we have broken the classification problem down into two separate stages Inference stage: learning a model for \\(p(\\mathcal{}C_k|\\mathbf{x})\\) from a set of training data Decision stage: making optimal class assignments for new data Alternatively we can combine the two stages and simply learn a discriminant function that maps inputs \\(\\mathbf{x}\\) directly into decisions. In fact, there are three approaches to solve decision problem, in decreasing order of complexity they are Generative model: explicitly or implicitly model the distribution of inputs as well as outputs, by sampling from it is possible to generate synthetic data points in the input space. It first solve the inference problem by determining \\(p(\\mathbf{x}|\\mathcal{C_k})\\) and \\(p(\\mathcal{C_k})\\), then we can infer the posterior and make decision from \\[ p(\\mathcal{C_k}|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})}{p(\\mathbf{x})} = \\frac{p(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})}{\\sum_kp(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})} \\] Discriminative model: solve the inference problem directly from learning the posterior \\(p(\\mathcal{C_k}|\\mathbf{x})\\) and then make decision. Discriminative function: combine both the inference and decision stage, learn a function that maps each input directly onto a class label. G model VS D model: Generative model is the most demanding and often need large data to ensure accuracy, but is useful for detecting new data with low probability, known as outlier detection or novelty detection. If we only need to make decisions for classification, we only need the \\(p(\\mathcal{C_k}|\\mathbf{x})\\) in discriminative model as shown below. The class-conditional density (or likelihood function) has no effect on posterior probabilities. Drawbacks of only D function: There are many reasons for wanting to compute the posterior, which can not be accessed in a single discriminative function Minimizing risk: no need to retrain the model if we only modify the loss matrix in decision stage. Reject option: depends on posterior. Compensating for class prior: for better generalization, compute posterior on modified data set and then compensate the effects of modification. (Details shown below) Combining models: combine posterior for small problems which is tackle by a separate model. (Details shown below) Details of \"compensating for class prior\": For example, cancer is rare and there may be only 1 in every 1000 training examples corresponds to cancer, our model can achieve 99.9% accuracy by simply assigning every point to the normal class, which is lack of generalization. One approach is to artificially select equal number of examples from both classes, but we have to compensate for this modification. The compensated posterior takes the form \\[ \\begin{aligned} p(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x}) &amp;= \\frac{p(\\mathbf{x}|\\mathcal{C_\\mathrm{cancer}})p(\\mathcal{C_\\mathrm{cancer}})}{p(\\mathbf{x})} \\\\&amp;= \\frac{p(\\mathbf{x}|\\mathcal{C_\\mathrm{cancer}})\\hat{p}(\\mathcal{C_\\mathrm{cancer}})}{p(\\mathbf{x})} \\frac{p(\\mathcal{C_\\mathrm{cancer}})}{\\hat{p}(\\mathcal{C_\\mathrm{cancer}})} \\\\&amp;= \\hat{p}(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x}) \\frac{p(\\mathcal{C_\\mathrm{cancer}})}{\\hat{p}(\\mathcal{C_\\mathrm{cancer}})} \\end{aligned} \\] where \\(\\hat{p}(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x})\\) is the posterior generated in balanced data set, and both the priors \\(p(\\mathcal{C_\\mathrm{cancer}})\\) and \\(\\hat{p}(\\mathcal{C_\\mathrm{cancer}})\\) can be interpreted as the fractions of points in each class. Details of \"combining models\": If the input spaces, for example, not only include the X-ray images but also blood tests, but now there are models using only one kind of data. Using conditional independent property we can combine these two models \\[ \\begin{aligned} p(\\mathcal{C_k}|\\mathbf{x_I}, \\mathbf{x_B}) &amp;\\propto p(\\mathbf{x_I}, \\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k}) \\\\ &amp;= p(\\mathbf{x_I}|\\mathcal{C_k}) p(\\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k}) \\\\ &amp;= \\frac{p(\\mathbf{x_I}|\\mathcal{C_k}) p(\\mathcal{C_k})p(\\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k})}{p(\\mathcal{C_k})} \\\\ &amp;= \\frac{ p(\\mathcal{C_k}|\\mathbf{x_I})p(\\mathbf{x_I}) p(\\mathcal{C_k}|\\mathbf{x_B})p(\\mathbf{x_B}) }{p(\\mathcal{C_k})} \\\\ &amp;\\propto \\frac{ p(\\mathcal{C_k}|\\mathbf{x_I}) p(\\mathcal{C_k}|\\mathbf{x_B}) }{p(\\mathcal{C_k})} \\end{aligned} \\] And finally we should normalize the new posteriors to ensure they sum to 1. Loss Functions for Regression For regression problem, the decision can be made similarly by minimizing the expected loss \\[ \\mathbb{E}[L] = \\int\\int L(t,y(\\mathbf{x}))p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\] A common choice for loss function in regression problem is \\(L(t,y(\\mathbf{x})) = [y(\\mathbf{x})-t]^2\\), and then minimize the \\(\\mathbb{E}[L]​\\) using the calculus of variations we have \\[ \\frac{\\delta\\mathbb{E}[L]}{\\delta y(\\mathbf{x})} = 2 \\int [y(\\mathbf{x})-t]p(\\mathbf{x},t) \\ \\mathrm{d}t = 0 \\] Then solving \\(y(\\mathbf{x})\\) from it \\[ y(\\mathbf{x}) = \\frac{\\int tp(\\mathbf{x},t)\\ \\mathrm{d}t}{p(\\mathbf{x})} = \\int tp(t|\\mathbf{x})\\ \\mathrm{d}t = \\mathbb{E}[t|\\mathbf{x}] \\] which is the conditional average of \\(t\\) conditioned on \\(\\mathbf{x}\\) and is known as regression function. An alternative way to derive this result is using the knowledge that the optimum is the conditional expectation. This approach shed light on the nature of the regression problem. First expand the squared loss term into \\[ \\begin{aligned} \\ [y(\\mathbf{x})-t]^2 &amp;= [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]+\\mathbb{E}[t|\\mathbf{x}]-t]^2 \\\\ &amp;= [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2+2[y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]][\\mathbb{E}[t|\\mathbf{x}]-t]+[\\mathbb{E}[t|\\mathbf{x}]-t]^2 \\end{aligned} \\] Substituting the into the loss function respectively we get for the first term \\[ \\begin{aligned} &amp;\\int \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\=&amp; \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 \\int p(\\mathbf{x},t)\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] For the second term, which is vanished \\[ \\begin{aligned} &amp;\\int \\int 2[y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]][\\mathbb{E}[t|\\mathbf{x}]-t]p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\= &amp; \\ 2\\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]p(\\mathbf{x})\\int[\\mathbb{E}[t|\\mathbf{x}]-t]p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp;\\ 2\\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]p(\\mathbf{x})*0\\ \\mathrm{d}\\mathbf{x} \\\\=&amp;\\ 0 \\end{aligned} \\] And for the last term \\[ \\begin{aligned} &amp;\\int \\int [\\mathbb{E}[t|\\mathbf{x}]-t]^2p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\ =&amp; \\int \\int [\\mathbb{E}^2[t|\\mathbf{x}]-2t\\mathbb{E}[t|\\mathbf{x}]+t^2]p(t|\\mathbf{x})p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\ =&amp; \\int \\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x})\\int p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} + \\int \\mathbb{E}[t|\\mathbf{x}]p(\\mathbf{x}) \\int -2tp(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} + \\int p(\\mathbf{x}) \\int t^2p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int \\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x})\\ \\mathrm{d}\\mathbf{x} + \\int -2\\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} + \\int \\mathbb{E}[t^2|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int \\left[\\mathbb{E}[t^2|\\mathbf{x}]-\\mathbb{E}^2[t|\\mathbf{x}]\\right]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int \\mathrm{var}[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] So finally we have \\[ \\mathbb{E}[L] = \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} + \\int \\mathrm{var}[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\] The function \\(y(\\mathbf{x})\\) we seek to determine only enters in the first term called bias, which will vanish if we choose \\(y(\\mathbf{x}) = \\mathbb{E}[t|\\mathbf{x}]\\), and the second term is called variance, which represents the noise during data generation and is irreducible. Similarly, there are three approaches to solve a regression problem, respectively by using: Generative model: learn a joint distribution \\(p(\\mathbf{x}, t)\\) then compute posterior \\(p(t|\\mathbf{x}) = p(\\mathbf{x}, t)/\\int p(\\mathbf{x}, t)\\ \\mathrm{d}t\\), and finally get \\(y(\\mathbf{x}) = \\int t p(t|\\mathbf{x})\\ \\mathrm{d}t\\). Discriminative model: only learn the posterior \\(p(t|\\mathbf{x}) = p(\\mathbf{x}, t)/\\int p(\\mathbf{x}, t)\\ \\mathrm{d}t\\), and then get \\(y(\\mathbf{x}) = \\int t p(t|\\mathbf{x})\\ \\mathrm{d}t\\). Discriminative function: Directly find \\(y(\\mathbf{x})\\) like what we did in multinomial curve fitting before. Sometimes like when \\(p(t|\\mathbf{x})\\) is multimodal, the square loss leads to poor results since \\(t\\) may less likely locates at \\(\\mathbb{E}[t|\\mathbf{x}]\\). We can consider a generalization of the square loss called Minkowski loss \\[ \\mathbb{E}[L_q]=\\int\\int |y(\\mathbf{x})-t|^q p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\] The minimum of \\(\\mathbb{E}[L_2]\\) is as before the conditional mean, and the minimum of \\(\\mathbb{E}[L_1]\\) is the conditional median, and as \\(q\\rightarrow 0\\), the minimum of \\(\\mathbb{E}[L_q]\\) is the conditional mode.","link":"/2018/04/30/PRML1-5/"},{"title":"Markov Decision Process","text":"Markov decision processes formally describe an environment for reinforcement learning, where the environment is fully observable. Markov Process Definition A Markov Process (or Markov Chain) is a memoryless random process (which satisfies Markov Property), denoted by a tuple \\(⟨\\mathcal{S},\\mathcal{P}⟩\\), where \\(\\mathcal{S}\\) is a (finite) set of states \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) State Transition Matrix State transition matrix \\(\\mathcal{P}\\) defines transition matrix probability from all states \\(s\\) to all successor states \\(s′\\), \\[ \\mathcal{P} = \\begin{bmatrix} \\mathcal{P}_{11} &amp; \\cdots &amp; \\mathcal{P}_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathcal{P}_{n1} &amp; \\cdots &amp; \\mathcal{P}_{nn} \\end{bmatrix} \\] where \\[ \\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s] \\] Note that each row of \\(\\mathcal{P}\\) sums to 1. A student Markov Chain example with its transition matrix. Markov Reward Process A Markov reward process is a Markov chain with values, denoted by a tuple \\(\\left\\langle \\mathcal{S,P,\\boldsymbol{R},\\boldsymbol{\\gamma}}\\right\\rangle\\) where \\(\\mathcal{S}\\) is a finite set of states \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) \\(\\boldsymbol{\\mathcal{R}}\\) is a reward function, \\(\\mathcal{R}_s=\\mathbb{E}[R_{t+1}|S_{t}=s]\\) \\(\\boldsymbol{\\mathcal{\\gamma}}\\) is a discount factor, \\(\\mathcal{\\gamma} \\in [0,1]\\) Student MRP, note that each reward is associated to a state, denoting the reward AFTER reaching that state Return The return \\(G_t\\) is the total discounted reward from time-step \\(t\\) \\[ G_t = R_{t+1}+\\gamma{R}_{t+2}+…=\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1} \\] Reasons of using the discount factor \\(\\gamma\\): Mathematically convenient to compute the discounted rewards Avoids infinite returns in cyclic Markov processes Uncertainty about the future may not be fully presented Focus more on immediate reward above delayed reward Human behavior shows preference for immediate reward Value Function The state value function \\(v(s)\\) of an MRP is a expected return starting from state \\(s\\) \\[ v(s) = \\mathbb{E}[G_t|S_t=s] \\] Bellman Equation State value function for Student MRP, using Bellman Equation, for example, \\(-5.0 = -2 + 0.9\\times (-7.6\\times 0.5+0.9\\times 0.5)\\) It can be easily shown that \\[ \\begin{aligned} v(s) &amp;= \\mathbb{E}[R_{t+1}+\\gamma{v}(S_{t+1})|S_t=s]\\\\ &amp;= \\mathcal{R}_s + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} \\mathcal{P}_{ss&#39;}v(s&#39;) \\end{aligned} \\] To express concisely into matrix form \\[ v = \\mathcal{R} + \\gamma{\\mathcal{P}}v \\] Then it can be solved directly since it’s a linear equation. Since the complexity of matrix inversion is \\(O(n^3)\\), direct solution is only possible for small MRPs. For large MRPs we need to use iterative methods. Markov Decision Process A Markov decision process (MDP) is a Markov reward process with decisions, denoted by a tuple \\(\\left\\langle \\mathcal{S,\\boldsymbol{A},P,R,\\gamma}\\right\\rangle\\) where \\(\\mathcal{S}\\) is a finite set of states \\(\\boldsymbol{\\mathcal{A}}\\) is a finite state of actions \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;}^\\boldsymbol{a} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s,A_t=\\boldsymbol{a}]\\) \\(\\mathcal{R}\\) is a reward function, \\(\\mathcal{R}_s^\\boldsymbol{a}=\\mathbb{E}[R_{t+1}|S_{t}=s,A_t=\\boldsymbol{a}]\\) \\(\\mathcal{\\gamma}\\) is a discount factor, \\(\\mathcal{\\gamma} \\in [0,1]\\) Policies A policy \\(\\pi\\) is a distribution over actions given states, fully defines the behavior of an agent \\[ \\pi{(a|s)} = \\mathbb{P}[A_t=a|S_t=s] \\] It can be easily shown that by definition, \\[ \\mathcal{P}_{ss&#39;}^{\\pi} = \\sum_{a\\in\\mathcal{A}}\\pi{(a|s)}\\mathcal{P}_{ss&#39;}^a \\\\ \\mathcal{R}_{s}^{\\pi} = \\sum_{a\\in\\mathcal{A}}\\pi{(a|s)}\\mathcal{R}_{s}^a \\] Value Function Different from the value function defined in MRP, we can define two kinds of value functions in MDP. The state value function \\(v_\\pi(s)\\) of an MDP is the expected return starting from state \\(s\\), and then following policy \\(\\pi\\) \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=t] \\] The action value function \\(q_\\pi(s)\\) of an MDP is the expected return starting from state \\(s\\), taking an action \\(a\\), and then following policy \\(\\pi\\) \\[ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t=t, A_t=a] \\] Bellman Expectation Equation Suppose an agent follows a specific policy \\(\\pi\\) (may not be deterministic). By decomposing state value function for one action onward, \\[ v_{\\pi}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} q_{\\pi}(s,a) \\] By decomposing action value function for one state onward, \\[ q_{\\pi}(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_\\pi(s&#39;) \\] Two steps onward for state value function, \\[ v_{\\pi}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_\\pi(s&#39;) \\right) \\] Two steps onward for action value function, \\[ q_{\\pi}(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a \\sum_{a&#39;\\in{\\mathcal{A}}} \\pi{(a&#39;|s&#39;)} q_{\\pi}(s&#39;,a&#39;) \\] Using the induced MRP (just follows the transition matrix and reward vector mentioned in the Policy section), the Bellman Equation can be expressed concisely, \\[ v_\\pi = \\mathcal{R^\\pi}+ \\gamma \\mathcal{P^{\\pi}}v_\\pi \\] which can also be solved directly with inversed matrix \\[ v_\\pi = (1- \\gamma \\mathcal{P^{\\pi}})^{-1}\\mathcal{R^\\pi} \\] Example Again, in the student MDP example, which is slightly modified from the student MRP, with \\(\\pi(a|s)=0.5\\) everywhere and \\(\\gamma=1\\). For the state value 7.4, it is computed by \\(0.5*(1-0.2*1.3+0.4*2.7+0.4*7.4)+0.5*10\\), which is a two steps onward computation. Optimal Value Functions The optimal state value function and optimal action value function over all policies, specifies the best possible performance in the MDP \\[ v_*(s) = \\max_{\\pi} v_{\\pi}(s)\\\\ q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a) \\] we say that an MDP is “solved” when the optimal value is known. Optimal Policy First define a partial ordering over all policies \\[ \\pi \\geq \\pi&#39; \\ \\ \\text{if}\\ \\ v_\\pi(s)\\geq v_{\\pi&#39;}(s), \\forall s \\] (Why is partial ordering? — There may be two policies that for some state \\(s\\), one value is greater than the other, but for some other state \\(s′\\) isn't.) then the following three theorems comes for any MDP, There exists an optimal policy \\(\\pi^*\\) that \\(\\pi^*\\geq \\pi\\), \\(\\forall\\pi\\) All optimal policies achieve the optimal state-value function, $ v_{^}(s) = v_(s) $ All optimal policies achieve the optimal action-value function, $q_{^}(s,a) = q_(s,a) $ There is always a deterministic optimal policy for any MDP, and if we know \\(q_*(s,a)\\), we immediately have the optimal policy \\[ \\pi_*(a|s) = \\left\\{ \\begin{align} 1 &amp; &amp; &amp;\\text{if} \\ \\ a = \\mathop{\\arg\\max}_{a\\in \\mathcal{A}} q_*(s,a) \\\\ 0 &amp; &amp;&amp; \\text{o.w.} \\end{align}\\right. \\] Bellman Optimality Equation Again, if some agent follows the optimal policy \\(\\pi_*\\), then from the theorem above we know it achieves both the optimal state value function and optimal action value function. By decomposing optimal action value function for one state onward, \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} q_*(s,a) \\] By decomposing optimal action value function for one state onward, \\[ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\] Two steps onward for optimal state value function, \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\right) \\] Two steps onward for optimal action value function, \\[ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a \\max_{a&#39;\\in{\\mathcal{A}}} q_*(s&#39;,a&#39;) \\] Example Again, for the student MDP example, the optimal state value function at red state is shown below: Solving the Bellman Optimality Equation Since the Bellman optimality equation is non-linear (maxmax op in it), there’s no closed form solution (in general). However, many iterative solution methods have been designed, and will be introduced in detail in the following notes: Value iteration Policy iteration Q-learning Sarsa Reference [1] David Silver's RL course","link":"/2018/09/23/RL_MDP/"},{"title":"Model-Free Prediction","text":"In this and next notes, different from the DP, we do all the things within an unknown MDP, just what the model-free means. Monte-Carlo Learning The basic idea is very simple, it uses the mean return of episodes as the true value. Learn from complete episodes, which refers to the procedure from current state to the final state. (The opposite learning method is bootstrapping, which means updating the current state value from incomplete episodes.) Restrict to the learning method, MC can only be applied to episodic MDPs, whose all episodes must terminates. MC is model-free, it has no knowledge of MDP transitions or rewards. In detail, MC's goal is to learn \\(v_\\pi\\) from episodes of experience under some policy \\(\\pi\\), like \\(S_1, A_1,R_2, ...,S_k \\sim \\pi\\), and using empirical mean-return instead of expected-return: \\[ v_\\pi(s) = \\mathbb{E}_\\pi [G_t|S=s] \\\\ \\text{estimated}~v_\\pi(s) = \\frac{1}{N(s)}\\sum_{i=1}^{N(s)} (G_{t})_i \\] where \\(G_t = R_{t+1}+\\gamma{R}_{t+2}+…\\gamma^{T-1}R_T\\) is the return of the whole episode. Basic Algorithm There are two slightly different strategies to update \\(v_\\pi(s)\\). Since within a single episode, one state may be visited more than one time, the strategies differ in whether to regard only the first visiting time as a beginning of an episode. In detail, one strategy, the first-visit Monte-Carlo policy evaluation for some state \\(s\\) is For the FIRST time-step \\(t​\\) that state \\(s​\\) is visited in an episode Increment counter \\(N(s) \\leftarrow N(s)+1​\\) and total return \\(S(s) \\leftarrow S(s)+G_t​\\) Value is evaluated by averaging over the total return \\(V(s) \\leftarrow S(s)/N(s) ​\\) Repeat steps 1-3, and by law of large numbers, \\(\\mathop{\\lim}_{N(s)\\rightarrow \\infty}V(s)=v_\\pi(s)\\) The other strategy, every-visit Monte-Carlo policy evaluation sum up the return of EVERY time-step \\(t​\\) that state \\(s​\\) is visited into total return \\(V(s)​\\). Example: Blackjack \\(\\mathcal{S}​\\) : \\(10\\times 10\\times 10=200​\\) in total current sum (12-21), if \\(sum &lt; 12​\\) just automatically twist Dealer's showing card (ace-10) whether have a \"usable\" ace (yes-no) \\(\\mathcal{A}\\) : 2 kinds of actions stick -- stop receiving cards twist -- take another card \\(\\mathcal{R}​\\) and \\(\\mathcal{P}​\\): if stick, game terminates, \\(R=+1,0,1\\) for sum of cards \\(&gt;,=,&lt;\\) dealer's cards, respectively if twist, \\(R=-1\\) when cards \\(&gt; 21\\) and game terminates, \\(R=0\\) otherwise. \\(\\mathcal{\\gamma}\\) : \\(0.9\\) For a very simple policy that stick if sum of cards \\(&gt;20​\\) (o.w. twist), the value function after MC learning is shown below Incremental Algorithm Since the mean \\(\\mu_1, \\mu_2, ...\\) of a sequence \\(x_1, x_2,...\\) can be computed incrementally, called incremental mean \\[ \\begin{align} \\mu_k &amp;= \\frac{1}{k} \\sum_{j=1}^k x_j \\\\ &amp;= \\frac{1}{k}\\left(x_k+ \\sum_{j=1}^{k-1} x_j\\right) \\\\ &amp;= \\frac{1}{k}(x_k+ (k-1)\\mu_{k-1} ) \\\\ &amp;= \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1} ) \\end{align} \\] Following this idea, we have a new update method. After one episode \\(S_1, A_1, R_2,...,S_T\\), for EVERY visited state \\(S_t\\) with return \\(G_t\\) \\(N(S_t) \\leftarrow N(S_t)+1\\) \\(V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t-V(S_t))\\) Now here comes our final idea, just like momentum is a modified version of SGD, this method keep tracking a running mean, by introducing a learning rate \\(\\alpha\\) to replace the term \\({1}/{N(S_t)}\\), so it forgets too old episodes, useful in non-stationary problems \\[ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t)) \\] Time-Difference Learning TD only differs from MC only in learning from incomplete episodes, by bootstrapping. Compare Incremental every-visit MC: \\(V(S_t) \\leftarrow V(S_t) + \\alpha(\\mathbf{G_t}-V(S_t))\\) Simplest TD algorithm, TD(0): \\[V(S_t) \\leftarrow V(S_t) + \\alpha(\\mathbf{R_{t+1} + \\gamma V(S_{t+1})}-V(S_t))\\] Here the estimated (not actual as in MC) return \\[R_{t+1} + \\gamma V(S_{t+1})\\] is call the TD target, \\[\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\] is called the TD error. Now Let's go through some examples to intuitively feel about the difference between the TD and MC. Example: Driving Home State Elapsed Time (min) Predicted Time to go Predicted Total Time leaving office 0 30 30 reach car, raining 5 35 40 exit highway 20 15 35 behind truck 30 10 40 home street 40 3 43 arrive home 43 0 43 MC vs. TD We can see from the first example that Advantages and Disadvantages of MC vs. TD -- 1 MC TD Learn before knowing final outcome No Yes Furthermore, Learn without final outcome No Yes Example: Random Walk Start from C, choose each direction with equal possibility, without discounting Training error between MC and TD with different learning rate. (Here are some results without any proof.) Advantages and Disadvantages of MC vs. TD -- 2 MC TD Variance high variance low variance Bias zero bias some bias Initial value not very sensitive to initial value sensitive to initial value Efficiency simple but time consuming more efficient than MC Convergence good approximation properties (even with function approximation) only TD(0) converges to \\(v_\\pi(s)\\) without bias (?) (but not always with function approximation under some special cases) Example: AB From the above we know that \\(V(s)\\rightarrow v_\\pi(s)\\) as experience \\(\\rightarrow \\infty\\), but what about we only obtain finite experience, namely, only some episodes (say \\(K\\) episodes) we have experienced, like \\[ s_1^1,a_1^1,r_2^1,...,s_{T_1}^1 \\\\ \\vdots \\\\ s_1^K,a_1^K,r_2^K,...,s_{T_K}^K \\] One possible method is to repeatedly sample the \\(K\\) episodes, feed them to find out the value function via MC or TD(0). Consider the situation that we experienced only two states A and B within 8 episodes, what value will we finally get for each state without discounting? By MC, we will get \\(V (A) = 0\\), since MC converges to solution with minimum mean-squared error, in other words, MC solution best fit to the observed returns \\[ \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}(G_t^k-V(s_t^k))^2 \\] By TD(0), we will get \\(V (A) = 3/4\\), since TD(0) converges to solution of max likelihood Markov model, in other words, TD(0) finds out an MDP that best fits the data \\[ \\hat{\\mathcal{P}}^a_{ss&#39;}=\\frac{1}{N(s,a)} \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\mathbf{1}(s_t^k,a_t^k,s_{t+1}^k=s,a,s&#39;) \\\\ \\hat{\\mathcal{R}}^a_{s}=\\frac{1}{N(s,a)} \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\mathbf{1}(s_t^k,a_t^k=s,a)r_t^k \\] Advantages and Disadvantages of MC vs. TD -- 3 MC TD Exploit Markov property No Yes Usually more efficient in Markov environments Usually more effective in non-Markov environments Unified View of DP, MC and TD MC backup: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t-V(S_t)) \\] TD backup: \\[ V(S_t) \\leftarrow V(S_t)+ \\alpha(R_{t+1} + \\gamma V(S_{t+1}-V(S_t))) \\] DP backup: \\[ V(S_t) \\leftarrow \\mathbb{E}_\\pi[R_{t+1}+\\gamma V(S_{t+1})] \\] Comparison: Bootstrapping (update involves an estimate) Sampling (update samples an expectation) MC No Yes TD Yes Yes DP Yes No Two dimensions stands for sampling depth and width, respectively. TD(\\(\\lambda\\)) Till now, the TD algorithm we introduced is actually TD(0), which only looks one step forward. What about looking 2 or more steps forward? n-Step Prediction First of all, define the n-step returns, \\[ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} +...+ \\gamma^{n-1} R_{t+n}+\\gamma^nV(S_{t+n}) \\] For example, \\[ \\begin{align} \\text{(TD)}~~~~~ G_t^{(1)} &amp;= R_{t+1} + \\gamma V(S_{t+1}) \\\\ G_t^{(2)} &amp;= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2}) \\\\ &amp;~~\\vdots \\\\ \\text{(MC)} ~~~ G_t^{(\\infty)} &amp;= R_{t+1} + \\gamma R_{t+2} + \\gamma^{T-1} R_T \\end{align} \\] And so we have the n-step temporal-difference learning \\[ V(S_t) \\leftarrow V(S_t)+ \\alpha(G_t^{(n)}-V(S_t)) \\] Example: Large Random Walk A basic problem from n-step TD algorithm is which n should we choose to achieve the best result? Researchers have made predictions under different step \\(n\\), different learning rate \\(\\alpha\\), and different training methods -- on-line(update the value after every step within an episode) and off-line (update values at the end of one episode). Two dimensions stands for sampling depth and width, respectively. We can see from the figure that the best \\(n\\) varies in different settings. In order to make an overall consideration, here we introduce a new parameter \\(\\lambda\\) to combine the information from all different time-steps. \\(\\lambda\\)-return The \\(\\lambda\\)-return \\(G_t^\\lambda\\) combines all \\(n\\)-step returns \\(G_t^{(n)}\\) by geometrically weighting them, which requires quite a little additional computation cost. \\[ G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)} \\\\ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^\\lambda- V(S_t)) \\] It can easily proved that by setting the last term's coefficient to \\(\\lambda^{T-t-1}\\) we have \\[ \\sum_{k=t+1}^{T-1} (1-\\lambda)\\lambda^{k-t-1} + \\lambda^{T-t-1} = 1 \\] Reference [1] David Silver's RL course","link":"/2018/10/01/RL_prediction/"},{"title":"ShadowsocksR资源整理","text":"由于一些众所周知的原因，我们并不能自由访问完整的互联网。但是互联网审查粒度过粗，导致许多本不需要审查的内容被拒之墙外，如Google Scholar，Wikipedia等。 ShadowsocksR是一种基于Socks5代理的加密传输协议，是shadowsocks的加强版本，可以帮助我们突破中国互联网审查(GFW)从而浏览被封锁的内容。ShadowsocksR分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并创建本地代理。 下面整理的是近期配置SSR时收集的一些资源。 服务端基本搭建流程 为了锐速的安装，Vultr购买系统为CentOS 6的服务器。尽量避免开到45.76, 208.开头的IP重灾区号段。 检验拿到的IP是否被封通过端口扫描，常见封22端口禁止ssh——TCP阻断。 服务端安装脚本基本参照【新手向】【梯子/代理】Vultr的购买+SSR+锐速+多端口的配置。但其中部分设置根据调研存在不合理之处。 更换内核以便安装锐速： 12345yum updaterpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpmrpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-2.6.32-504.3.3.el6.x86_64.rpm --forcerpm -qa | grep kernel # 确认内核是否更换成功,当看到有kernel-2.6.32-504.3.3.el6.x86_64就说明更换成功了reboot 安装SSR客户端，以及个人推荐使用的设置： 1234567891011121314wget -N --no-check-certificate http://www.aloneray.com/wp-content/uploads/2018/09/shadowsocksR.sh &amp;&amp; bash shadowsocksR.sh# port: 80# protocol: auth_chain_a# encrpt: none# obfs: plain/simple_http# ================= 常用指令 =================# 启动：/etc/init.d/shadowsocks start# 停止：/etc/init.d/shadowsocks stop# 重启：/etc/init.d/shadowsocks restart# 状态：/etc/init.d/shadowsocks status# 卸载：./shadowsocksR.sh uninstall# 配置文件路径： /etc/shadowsocks.json 首先18年3月有推文指出SSR的tls凭据复用已经成为安全问题，即发布于17年8月后长期未更新的ShadowsocksR 协议插件文档中强烈推荐使用的混淆方法tls1.2_ticket_auth已不再能使用。亲测使用后48小时内被封。 端口理论上应当使用80/443分别对应需要伪装成的http/https流量。若使用simple_http混淆则应使用80端口。 混淆参数的设置参见SSR混淆及混淆协议参数的设置，http_simple可以自定义几乎完整的http header。 auth_chain_a基本是目前 SSR 最佳的稳定版协议，根据ShadowsocksR 协议插件文档应对应使用无加密none。 安装锐速加速，实测东京节点锐速效果要优于BBR，这也是不使用Vultr上自带BBR的Debian 9等系统的原因： 123456789wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.shservice iptables stop # 关闭防火墙chkconfig iptables off# ================= 常用指令 =================# 重启：/serverspeeder/bin/serverSpeeder.sh restart# 启动：/serverspeeder/bin/serverSpeeder.sh start# 停止：/serverspeeder/bin/serverSpeeder.sh stop# 状态：/serverspeeder/bin/serverSpeeder.sh status 客户端安装 Platform URL Windows https://github.com/shadowsocksr-backup/shadowsocksr-csharp/releases MacOS https://github.com/shadowsocksr-backup/ShadowsocksX-NG/releases Android https://github.com/shadowsocksr-backup/shadowsocksr-android/releases 针对普通用户的使用教程详见[ShadowsocksR] 大概是萌新也看得懂的SSR功能详细介绍&amp;使用教程，十分详尽。 Linux下参考在Linux的环境安装shadowsocksR客户端： 12345678# 安装wget https://onlyless.github.io/ssrsudo mv ssr /usr/local/binsudo chmod 766 /usr/local/bin/ssrssr install# 配置与启动ssr configssr start 为了方便使用还需要设置开机自动启动ssr。不过ubuntu 18.04不能像ubuntu 14一样通过编辑rc.local来设置开机启动脚本，通过下列简单设置后，可以使rc.local重新发挥作用。参考ubuntu-18.04 设置开机启动脚本进行设置： 建立rc-local.service文件，sudo vim /etc/systemd/system/rc-local.service，复制进以下内容： 1234567891011121314[Unit]Description=/etc/rc.local CompatibilityConditionPathExists=/etc/rc.local [Service]Type=forkingExecStart=/etc/rc.local startTimeoutSec=0StandardOutput=ttyRemainAfterExit=yesSysVStartPriority=99 [Install]WantedBy=multi-user.target 创建文件rc.local，sudo vim /etc/rc.local，将下列内容复制进去： 1234567891011121314#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will &quot;exit 0&quot; on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.sudo ssr startexit 0 给rc.local加上权限，启用服务，检查状态 1234sudo chmod +x /etc/rc.localsudo systemctl enable rc-localsudo systemctl start rc-local.servicesudo systemctl status rc-local.service GenPac的使用 使用系统的自动代理功能，实现墙内外流量分流，科学上网。GenPac是基于gfwlist的多种代理软件配置文件生成工具。安装后根据gfwlist生成代理规则.pac文件，最后设置系统根据该.pac自动代理即可。Linux下的一般安装步骤： 12pip install -U genpacgenpac --proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" -o autoproxy.pac --gfwlist-url=\"https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\" # 生成代理规则文件 其他平台可直接根据gfwlist.txt使用在客户端中使用pac模式。 其他资源链接汇总 参照SS-and-SSR-Collection，这里备份用再写一遍。 Shadowsocks Platform URL Windows https://github.com/shadowsocks/shadowsocks-windows/releases MacOS https://github.com/shadowsocks/ShadowsocksX-NG/releases Android https://github.com/shadowsocks/shadowsocks-android/releases obfs https://github.com/shadowsocks/simple-obfs-android/releases SSTap 用于非http流量的代理，主要用于游戏。https://www.sockscap64.com/sstap-enjoy-gaming-enjoy-sstap/ Rules Rule URL SSTap https://github.com/FQrabbit/SSTap-Rule GFWList https://github.com/gfwlist/gfwlist ChinaList https://github.com/felixonmars/dnsmasq-china-list PAC https://github.com/breakwa11/gfw_whitelist chnrouter IP URL IPIP https://raw.githubusercontent.com/17mon/china_ip_list/master/china_ip_list.txt APNIC curl 'http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest' | grep ipv4 | grep CN | awk -F\\| '{ printf(\"%s/%d\\n\", $4, 32-log($5)/log(2)) }' &gt; chnroute.txt DNS Reference: 域名服务器缓存污染。 DNS URL ChinaDNS https://github.com/shadowsocks/ChinaDNS Pcap DNSProxy https://github.com/chengr28/Pcap_DNSProxy overture https://github.com/shawn1m/overture","link":"/2018/12/21/ssr/"}],"tags":[{"name":"PRML","slug":"PRML","link":"/tags/PRML/"},{"name":"RL","slug":"RL","link":"/tags/RL/"},{"name":"memo","slug":"memo","link":"/tags/memo/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"ssr","slug":"ssr","link":"/tags/ssr/"}],"categories":[]}