{"pages":[],"posts":[{"title":"Hexo配置备忘","text":"Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Minos is a simple and retro styled Hexo theme, concentrating more on your ideas. 基本流程 hexo建站基本配置直接参照官网步骤，配置主题为Minos。 公式支持 网上广为流传的替换hexo默认渲染为hexo-renderer-kramed的方法，仍然会存在行内公式含两个引号时中间部分被渲染为斜体的问题。 最后参照Hexo 书写 LaTeX 公式时的一些问题及解决方法，替换默认渲染为hexo-renderer-pandoc。 -- 注意1：事先去pandoc官方下载最新版本的pandoc，否则直接使用apt install pandoc可能在run服务时因为版本过低出现pandoc: Unknown extension: smart的问题 -- 注意2：替换完成后主题内的依赖关系不会自动更改，需要将themes/minos/scripts/01_check.js中对默认渲染的依赖项'hexo-renderer-marked'手动替换为'hexo-renderer-pandoc' 图像居中 为了美观（强迫症），修改themes/source/css/style.scss中display属性为block，添加属性text-align: center。","link":"/2018/01/12/hexo-install/"},{"title":"(PRML Notes) 1.1 Polynomial Curve Fitting as an Introductory Example","text":"A series of notes taken from Pattern Recognition and Machine Learning. This introduction chapter is mainly composed of these three theories: Probability theory provides a framework for expressing uncertainty in a precise and quantitative manner. Decision theory allows us to exploit probabilistic representation in order to make predictions that are optimal. Information theory studies the quantification, storage, and communication of information. Example: Polynomial Curve Fitting Consider a given data set \\(\\mathbf{x} \\equiv (x_1, ..., x_N)^T\\) and with the corresponding values \\(\\mathbf{t} \\equiv (t_1, ..., t_N)^T\\), where the values are generated by \\[ t_i = \\sin(2\\pi x_i) + \\epsilon, \\ i= 1,2,...,N \\] Here \\(\\epsilon\\) is a small random noise following a Gaussian distribution. A simple approach of curve fitting is to fit the data using a polynomial function \\[ y(x, \\mathbf{w}) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M = \\sum_{j=0}^M w_j x^j \\] It is not a linear function of \\(x\\), but linear in \\(\\mathbf{w}\\) by viewing \\(x^j\\) as the parameter of \\(w_j\\). The curve fitting can be done by minimizing an error function like \\[ E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 \\] Since the error function is quadratic of \\(\\mathbf{w}\\), the minimization of \\(E(\\mathbf{w})\\) has a unique solution \\(\\mathbf{x}^*\\). To compare different sizes of data sets on an equal footing, it is sometimes more convenient to use the root-mean-square(RMS) error \\[ E_{RMS} = \\sqrt{2E(\\mathbf{w}^*)/N} \\] However, minimizing the error is not the same thing as getting a better model. if our model is more complex, for example if \\(M\\) is too large, we may suffer from the over-fitting problem, namely our model will perform much worse on unseen data. Over-fitting problem. Generally speaking, I think the most data generated from nature follows the law of simplicity, something like the Occam's Razor, so one should select the model that makes the fewest assumptions. This is expressed as prior knowledge later in Bayesian probability later. One approach to avoid over-fitting is regularization, adding a penalty term to the error function, for example \\[ \\widetilde{E}(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\] Note that often the coefficient \\(w_0\\) is omitted from the regularization because its inclusion causes the results to depend on the choice of origin for the target variable. It may be included but with its own regularization coefficient. Perhaps under many circumstances, the data may be generated from some rule like \\(t_i = \\sin(2\\pi x_i) + b + \\epsilon\\), not simply as \\(t_i = \\sin(2\\pi x_i) + \\epsilon\\), and we should learn the natural bias instead of penalize it. Such techniques are known as shrinkage in statistics literature, the particular case of a quadratic regularizer is called ridge regression, and in the context of neural networks, this approach is known as weight decay. Selection of Models with different complexity To determine the suitable value of model complexity (either \\(M\\) or \\(\\lambda\\) here in the context), we can partition the available data into a training set, used to determine the coefficients \\(\\mathbf{w}\\), and a separate validation set, or called a hold-out set, used to optimize the model complexity.","link":"/2018/04/22/PRML1-1/"},{"title":"Lagrange Multiplier and KKT Conditions","text":"In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maximum and minimum of a function subject to equality constraints, while KKT conditions solve the problems with inequality constraints. Naive Approach Consider the problem of finding the maximum of a function \\(f(x_1, x_2)\\) subject to a constraint relating \\(x_1\\) and \\(x_2\\), which we write in the form \\[ g(x_1, x_2) = 0 \\] One approach would be to solve the above equation to express \\(x_2\\) as a function of \\(x_1\\) in the form of \\(x_2 = h(x_1)\\), then apply differentiation to \\(f(x_1, h(x_1))\\) w.r.t. \\(x_1\\) in the usual way. However, there are at least two drawbacks of this simple approach: Sometimes difficult or even impossible to find out an expression like \\(x_2 = h(x_1)\\) Spoil the natural symmetric between these variables A more elegant way is introducing a parameter \\(\\lambda\\) called a Lagrange multiplier. Lagrange Multiplier An Intuitive Understanding Now consider in a \\(D\\)-dimensional space where we need to \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = 0 \\] Geometrically, the constraint \\(g(\\mathbf{x}) = 0\\) represents a \\((D-1)\\)-dimensional surface, and what's worth noting is that at any point on the constraint surface, the gradient \\(\\nabla g(\\mathbf{x})\\) will be orthogonal to the surface. To see this, consider the total differential \\(dg = \\nabla g(\\mathbf{x})^T d \\mathbf{x}\\), then if the point \\(\\mathbf{x}+d\\mathbf{x}\\) also lies on the constraint surface, then \\(dg=0\\), so we have \\(\\nabla g(\\mathbf{x})^T d\\mathbf{x} = 0\\). Next, the point \\(\\mathbf{x}^*\\) on the constraint surface that maximize \\(f(\\mathbf{x})\\) must have the property that \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), in other words, \\(\\nabla g(\\mathbf{x}^*)\\) is orthogonal to the constraint surface, because otherwise we could increase the value of \\(f(\\mathbf{x})\\) by moving a short distance along the constraint surface. Personally, I think it's quite similar to the force resolution, where we can view the point \\(\\mathbf{x}\\) as a ball on its orbit -- the constraint surface. A force, \\(\\nabla f(\\mathbf{x})\\), is applied on it and the ball will finally moves up to a place where no force can be resolved on the tangent plane of the place. Since \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), there must exist a parameter \\(\\lambda\\) such that \\[ \\nabla f(\\mathbf{x}^*) +\\lambda \\nabla g(\\mathbf{x^*}) = 0 \\] where \\(\\lambda\\neq 0\\) is known as Lagrange multiplier. For representation convenience, we can introduce the Lagrangian function defined by \\[ L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) \\] By setting \\(\\nabla_{\\mathbf{x}} L=\\mathbf{0}\\) together with the constraint \\(g(\\mathbf{x}) = 0\\), we can get all the necessary conditions (but not sufficient) when \\(\\mathbf{x}\\) is an extreme point on the constraint surface. Example 1 \\[ \\max \\ 1-x_1^2 - x_2^2 \\\\ \\text{s.t.} \\ \\ x_1 + x_2 -1 =0 \\] The corresponding Lagrange function is given by \\(L(\\mathbf{x}, \\lambda) = 1-x_1^2 - x_2^2 + \\lambda (x_1 + x_2 -1)\\), then set \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_1} &amp;= -2x_1 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial x_2} &amp;= -2x_2 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial \\lambda} &amp;= x_1 + x_2 -1 = 0 \\end{aligned} \\] By solving the system of equations we get the stationary point on the constraint surface is \\((\\frac{1}{2},\\frac{1}{2})\\), and it can be verified that it is a maximum point by checking nearby points or using second-order sufficient conditions (see further reading 2). KKT Conditions So far we have considered the optimization problem with equality constraint only. Now consider such problem with inequality constraint \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) \\geq 0 \\] The solution of this problem can be classified into two kinds: When the constraint is active, that is when the stationary point lies on the boundary \\(g(\\mathbf{x})=0\\), the problem is just analogous to the one discussed previously and corresponds to a stationary point with \\(\\lambda \\neq 0\\). Note that now the sign of \\(\\lambda\\) is crucial, since \\(f(\\mathbf{x})\\) will only be at a maximum if \\(\\nabla f(\\mathbf{x})\\) is away from the region \\(g(\\mathbf{x})&gt;0\\). So further we have the corresponding \\(\\lambda &gt; 0\\). When the constraint is inactive, the stationary point lies in the region \\(g(\\mathbf{x})&gt;0\\), with corresponding \\(\\lambda = 0\\). Again for the convenience of representation, note that for both cases, the product \\(\\lambda g(\\mathbf{x})=0\\), thus the solution can be obtained by maximizing the Lagrange function w.r.t. \\(\\mathbf{x}\\) s.t. the conditions \\[ \\begin{aligned} g(\\mathbf{x}) &amp;\\geq 0 &amp; &amp; \\text{(constriant function)} \\\\ \\lambda &amp;\\geq 0&amp;&amp; \\text{(gradient direction)} \\\\ \\lambda g(\\mathbf{x})&amp;=0&amp;&amp; \\text{(complementary slackness condition)} \\end{aligned} \\] These are known as Karush-Kuhn-Tucker(KKT) conditions. If we need to apply KKT conditions to minimization problems, just change the sign of \\(\\lambda\\) in Lagrange function to keep the gradient direction reversed, i.e. \\(L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\\) ###Multiple Equality and Inequality Constraints Finally, it's now easy to extend the method to the case of multiple equality and inequality constraints \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\ \\ \\ \\ \\ \\ h_j(\\mathbf{x}) =0, \\ j=1,...,l \\] Define the Lagrange function to be \\[ L(\\mathbf{x},\\mathbf{v},\\mathbf{w}) = f(\\mathbf{x}) + \\sum_{i=1}^m w_i g_i(\\mathbf{x}) + \\sum_{j=1}^l v_j h_j(\\mathbf{x}) \\] Now the necessary conditions when a point \\(\\mathbf{x}\\) becomes the local optimum is \\[ \\left\\{\\begin{aligned} &amp; \\nabla_x L(\\mathbf{x},\\mathbf{w},\\mathbf{v}) = 0 \\\\&amp;g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\&amp;h_j(\\mathbf{x}) = 0, \\ j=1,...,l \\\\&amp; w_i \\geq 0,\\ i=1,...,m \\\\&amp; w_i g(\\mathbf{x})=0,\\ i=1,...,m \\end{aligned}\\right. \\] Example 2 \\[ \\min \\ x_1^2 - x_2 - 3x_3 \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = -x_1 - x_2 - x_3 \\geq 0 \\\\ \\ \\ \\ \\ \\ \\ h(\\mathbf{x}) = x_1^2 + 2x_2 - x_3 = 0 \\] Write down Lagrange function for minimization problem \\[ L(\\mathbf{x}, w, v) = (x_1^2 - x_2 - 3x_3) - w(-x_1 - x_2 - x_3) - v(x_1^2 + 2x_2 - x_3 ) \\] As described above, the first-order necessary condition of optimum is \\[ \\left\\{\\begin{aligned} &amp;L_{x_1}&#39; = 2x_1 + w - 2vx_1 =0 \\\\ &amp;L_{x_2}&#39; = -1 + w - 2v=0 \\\\ &amp;L_{x_3}&#39; = -3 + w +v=0 \\\\ &amp;-x_1 - x_2 - x_3 \\geq 0 \\\\ &amp; x_1^2 + 2x_2 - x_3 = 0 \\\\&amp;w\\geq 0 \\\\&amp; w(-x_1 - x_2 - x_3) = 0 \\end{aligned}\\right. \\] The solution is \\(\\mathbf{x}^* = (-\\frac{7}{2}, -\\frac{35}{12}, \\frac{77}{12}), \\ w = \\frac{7}{3}, \\ v=\\frac{2}{3}\\). Further Reading Lagrange Multipliers Can Fail To Determine Extrema In single variable case, when the gradient of the constraint function \\(\\nabla g(\\mathbf{x})=0\\), the simple Lagrangian multiplier discussed above doesn't work. e.g. \\(\\min \\ x \\text{, s.t.} \\ \\ g(x,y) = y^2 + x^4 - x^3=0\\). http://web.cs.iastate.edu/~cs577/handouts/lagrange-multiplier.pdf Explain Lagrange multiplier in multi-constraint case in detail, and give the second-order sufficient conditions, which use the definitive property of Lagrangian function's Hessian on the tangent space to verify if a stationary point is also an extreme point in a more mathematical way. Besides, it avoids discussing the cases like that mentioned in the above article by introducing the concept of regular point. Tangent space at x^* on a surface defined by (a) one constraint (shown as a tangent plane) and (b) two constraints (shown as a tangent line). According to the second-order sufficient conditions mentioned in further reading 2, we can verifying the solution as optimum in example 2 by first calculating the Hessian at this point \\[ \\nabla_{\\mathbf{x}}^2L(\\mathbf{x}^*, w, v) = \\begin{bmatrix} \\frac{2}{3} &amp;0&amp; 0 \\\\0&amp;0&amp;0\\\\0&amp;0&amp;0 \\end{bmatrix} \\] Since both the two constraints are active at \\(\\mathbf{x^*}\\), we can solving the systems of equations below to get the expression of the tangent space at this point \\[ \\left\\{ \\begin{aligned} \\nabla g(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\\\ \\nabla h(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\end{aligned} \\right. \\] where \\(\\mathbf{d} = (d_1, d_2, d_3)^T\\) stands for any feasible direction on the tangent space, and \\(g(\\mathbf{x}^*) = (-1,-1,-1)^T\\), \\(h(\\mathbf{x}^*) = (-7,2,-1)\\). By solving that we have \\(\\mathbf{d} = (d_1,2d_1,-3 d_1)^T\\). Because \\(\\mathbf{d}^T\\nabla^2_{\\mathbf{x}}L(\\mathbf{x}^*, w, v) \\mathbf{d}=\\frac{2}{3}d_1^2&gt;0\\), any little change away from \\(\\mathbf{x}^*\\) always makes \\(L\\) increase, the objective function really reaches local minimum at the point \\(\\mathbf{x}^*\\), which is also the global minimum point as well since there is no other local minimum. References [1] Wikipedia: Lagrange multiplier [2] Wikipedia: Karush–Kuhn–Tucker conditions [3] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix E. New York: Springer.","link":"/2018/04/21/KKT/"},{"title":"Calculus of Variations","text":"Calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maximum and minimum of functionals. Introduction Functional is the function's function, in other words, functional takes a function as input and outputs a real number, or more mathematically, it refers to a mapping from a space \\(X\\) (commonly \\(X\\) is a space of functions) into the real numbers. In the calculus of variations, we seek a functional \\(f(x)\\) that maximizes (or minimizes) a functional \\(F[f]\\). For example, as we all know, the shortest curve between two fixed points is the straight line, and this can be proved using calculus of variations. Example 1: Shortest Curve To find out the shortest curve \\(y(x)\\) between two fixed points \\((a_1, b_1)\\) and \\((a_2, b_2)\\), we first express the length of the curve in the form of integration \\[ F[y] = \\int_{a_1}^{a_2} \\sqrt{1+[y&#39;(x)]^2} dx \\] where we have the boundary conditions of \\(y(x)\\) \\[ y(a_1) = b_1,\\ y(a_2) = b_2 \\] Suppose \\(y(x)\\) is twice continuously differentiable. Now if the functional \\(F\\) attains a local minimum at \\(y=f\\), consider \\(\\eta(x)\\) to be an arbitrary function that has at least one derivative and vanishes at end points \\(a_1\\) and \\(a_2\\), i.e. \\(\\eta(a_1) = 0\\) and \\(\\eta(a_2)=0\\), then we have \\[ F[f] \\leq F[f + \\epsilon\\eta] \\] where \\(\\epsilon\\) can be any number close to \\(0\\), here the term \\(\\epsilon \\eta\\) is called the variation of the function \\(f\\) and is denoted by \\(\\delta f\\). We can view \\(F[f+\\epsilon \\eta]\\) as a function of \\(\\epsilon\\) and denote it as \\(J(\\epsilon)\\), then \\(J&#39;(0) = 0\\) must be true for any arbitrary choice of \\(\\eta\\) since the functional \\(F[y]\\) achieves minimum when \\(y=f\\). By calculating \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\left. \\frac{d}{d\\epsilon}F[f+\\epsilon\\eta] \\right|_{\\epsilon=0} \\\\ &amp;= \\left. \\frac{d}{d\\epsilon} \\int_{a_1}^{a_2} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}dx \\right|_{\\epsilon=0} \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{d}{d\\epsilon} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{[f&#39;(x) + \\epsilon\\eta&#39;(x)]\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx \\end{aligned} \\] We get \\[ \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx = 0 \\] which means that in differentiable function space, \\(F[y]\\)'s derivatives in all directions (all \\(\\eta\\)) are zero. Since \\(y(x)\\) is twice continuously differentiable, we can further integrate the left hand side of above equation by parts \\[ \\begin{aligned} \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx &amp;= \\left. \\eta(x) \\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right|_{a_1}^{a_2} -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx \\\\ &amp;= -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx\\ \\ \\ \\text{(since $\\eta(a_1) = 0$ and $\\eta(a_2) = 0$)} \\end{aligned} \\] then now we have \\[ \\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx = 0 \\] Since \\(\\eta(x)\\) is arbitrary, imagine choosing a perturbation \\(\\eta(x)\\) that is zero everywhere except in the neighborhood of a point \\(\\hat{x}\\), in which case the functional derivative must be zero at \\(x=\\hat{x}\\), otherwise the integral will not be equal to zero. However, because this must be true for every choice of \\(\\hat{x}\\), the functional derivative must vanish for all values of \\(x\\), that is \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = 0 \\] This is called the fundamental lemma of calculus of variations. Follow this result we have \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = \\frac{f&#39;&#39;(x)}{(1 + [f&#39;(x)]^2)^{\\frac{3}{2}}} = 0 \\] So \\[ f&#39;&#39;(x)=0 \\] Euler–Lagrange Equation Now consider the more general case, for the functional \\[ F[y] = \\int_{x_1}^{x_2} L(x,y(x), y&#39;(x)) dx \\] Again suppose the functional attains a local minimum at \\(y=f\\), and define \\(J(\\epsilon) = F[f+\\epsilon\\eta]\\). Then we have \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\int_{x_1}^{x_2} \\left. \\frac{dL}{d\\epsilon} \\right|_{\\epsilon=0}dx \\\\ &amp;= \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f}\\eta +\\frac{\\partial L}{\\partial f&#39;}\\eta&#39; \\right) dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx +\\left.\\frac{\\partial L}{\\partial f&#39;}\\eta \\ \\right|_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\eta \\left(\\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\right) dx \\end{aligned} \\] According to the fundamental lemma of calculus of variations, we get \\[ \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] = 0 \\] which is called the Euler–Lagrange equation. The left hand side of this equation is called the functional derivative of \\(F[f]\\), denoted as \\[ \\frac{\\delta J}{\\delta f} = \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\] References [1] Wikipedia: Calculus of variations [2] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix D. New York: Springer.","link":"/2018/04/18/variations/"},{"title":"ShadowsocksR资源整理","text":"由于一些众所周知的原因，我们并不能自由访问完整的互联网。但是互联网审查粒度过粗，导致许多本不需要审查的内容被拒之墙外，如Google Scholar，Wikipedia等。 ShadowsocksR是一种基于Socks5代理的加密传输协议，是shadowsocks的加强版本，可以帮助我们突破中国互联网审查(GFW)从而浏览被封锁的内容。ShadowsocksR分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并创建本地代理。 下面整理的是近期配置SSR时收集的一些资源。 服务端基本搭建流程 为了锐速的安装，Vultr购买系统为CentOS 6的服务器。尽量避免开到45.76, 208.开头的IP重灾区号段。 检验拿到的IP是否被封通过端口扫描，常见封22端口禁止ssh——TCP阻断。 服务端安装脚本基本参照【新手向】【梯子/代理】Vultr的购买+SSR+锐速+多端口的配置。但其中部分设置根据调研存在不合理之处。 更换内核以便安装锐速： 12345yum updaterpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpmrpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-2.6.32-504.3.3.el6.x86_64.rpm --forcerpm -qa | grep kernel # 确认内核是否更换成功,当看到有kernel-2.6.32-504.3.3.el6.x86_64就说明更换成功了reboot 安装SSR客户端，以及个人推荐使用的设置： 1234567891011121314wget -N --no-check-certificate http://www.aloneray.com/wp-content/uploads/2018/09/shadowsocksR.sh &amp;&amp; bash shadowsocksR.sh# port: 80# protocol: auth_chain_a# encrpt: none# obfs: plain/simple_http# ================= 常用指令 =================# 启动：/etc/init.d/shadowsocks start# 停止：/etc/init.d/shadowsocks stop# 重启：/etc/init.d/shadowsocks restart# 状态：/etc/init.d/shadowsocks status# 卸载：./shadowsocksR.sh uninstall# 配置文件路径： /etc/shadowsocks.json 首先18年3月有推文指出SSR的tls凭据复用已经成为安全问题，即发布于17年8月后长期未更新的ShadowsocksR 协议插件文档中强烈推荐使用的混淆方法tls1.2_ticket_auth已不再能使用。亲测使用后48小时内被封。 端口理论上应当使用80/443分别对应需要伪装成的http/https流量。若使用simple_http混淆则应使用80端口。 混淆参数的设置参见SSR混淆及混淆协议参数的设置，http_simple可以自定义几乎完整的http header。 auth_chain_a基本是目前 SSR 最佳的稳定版协议，根据ShadowsocksR 协议插件文档应对应使用无加密none。 安装锐速加速，实测东京节点锐速效果要优于BBR，这也是不使用Vultr上自带BBR的Debian 9等系统的原因： 123456789wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.shservice iptables stop # 关闭防火墙chkconfig iptables off# ================= 常用指令 =================# 重启：/serverspeeder/bin/serverSpeeder.sh restart# 启动：/serverspeeder/bin/serverSpeeder.sh start# 停止：/serverspeeder/bin/serverSpeeder.sh stop# 状态：/serverspeeder/bin/serverSpeeder.sh status 客户端安装 Platform URL Windows https://github.com/shadowsocksr-backup/shadowsocksr-csharp/releases MacOS https://github.com/shadowsocksr-backup/ShadowsocksX-NG/releases Android https://github.com/shadowsocksr-backup/shadowsocksr-android/releases 针对普通用户的使用教程详见[ShadowsocksR] 大概是萌新也看得懂的SSR功能详细介绍&amp;使用教程，十分详尽。 Linux下参考在Linux的环境安装shadowsocksR客户端： 12345678# 安装wget https://onlyless.github.io/ssrsudo mv ssr /usr/local/binsudo chmod 766 /usr/local/bin/ssrssr install# 配置与启动ssr configssr start 为了方便使用还需要设置开机自动启动ssr。不过ubuntu 18.04不能像ubuntu 14一样通过编辑rc.local来设置开机启动脚本，通过下列简单设置后，可以使rc.local重新发挥作用。参考ubuntu-18.04 设置开机启动脚本进行设置： 建立rc-local.service文件，sudo vim /etc/systemd/system/rc-local.service，复制进以下内容： 1234567891011121314[Unit]Description=/etc/rc.local CompatibilityConditionPathExists=/etc/rc.local [Service]Type=forkingExecStart=/etc/rc.local startTimeoutSec=0StandardOutput=ttyRemainAfterExit=yesSysVStartPriority=99 [Install]WantedBy=multi-user.target 创建文件rc.local，sudo vim /etc/rc.local，将下列内容复制进去： 1234567891011121314#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will &quot;exit 0&quot; on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.sudo ssr startexit 0 给rc.local加上权限，启用服务，检查状态 1234sudo chmod +x /etc/rc.localsudo systemctl enable rc-localsudo systemctl start rc-local.servicesudo systemctl status rc-local.service GenPac的使用 使用系统的自动代理功能，实现墙内外流量分流，科学上网。GenPac是基于gfwlist的多种代理软件配置文件生成工具。安装后根据gfwlist生成代理规则.pac文件，最后设置系统根据该.pac自动代理即可。Linux下的一般安装步骤： 12pip install -U genpacgenpac --proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" -o autoproxy.pac --gfwlist-url=\"https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\" # 生成代理规则文件 其他平台可直接根据gfwlist.txt使用在客户端中使用pac模式。 其他资源链接汇总 参照SS-and-SSR-Collection，这里备份用再写一遍。 Shadowsocks Platform URL Windows https://github.com/shadowsocks/shadowsocks-windows/releases MacOS https://github.com/shadowsocks/ShadowsocksX-NG/releases Android https://github.com/shadowsocks/shadowsocks-android/releases obfs https://github.com/shadowsocks/simple-obfs-android/releases SSTap 用于非http流量的代理，主要用于游戏。https://www.sockscap64.com/sstap-enjoy-gaming-enjoy-sstap/ Rules Rule URL SSTap https://github.com/FQrabbit/SSTap-Rule GFWList https://github.com/gfwlist/gfwlist ChinaList https://github.com/felixonmars/dnsmasq-china-list PAC https://github.com/breakwa11/gfw_whitelist chnrouter IP URL IPIP https://raw.githubusercontent.com/17mon/china_ip_list/master/china_ip_list.txt APNIC curl 'http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest' | grep ipv4 | grep CN | awk -F\\| '{ printf(\"%s/%d\\n\", $4, 32-log($5)/log(2)) }' &gt; chnroute.txt DNS Reference: 域名服务器缓存污染。 DNS URL ChinaDNS https://github.com/shadowsocks/ChinaDNS Pcap DNSProxy https://github.com/chengr28/Pcap_DNSProxy overture https://github.com/shawn1m/overture","link":"/2018/12/21/ssr/"},{"title":"(PRML Notes) 1.2 Probability Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. Probability Theory Due to the finite size of data sets and the noise on measurements, we use probability to express the uncertainty in pattern recognition. Some concepts like sum rule, product rule, marginal probability, joint probability, conditional probability are very basic, but for this book it would be worthy of gaining insight into Bayes theorem, which plays a central role in PRML \\[ p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} \\] Using sum rule, the denominator, which can be viewed as a normalizer, can also be expressed in terms of quantities appearing in the numerator \\[ p(X) = \\sum_Y p(X|Y)p(Y) \\] Probability Densities If the probability of a real-valued variable \\(x\\) falling in the interval \\((x, x+\\delta x)\\) is given by \\(p(x)\\delta x\\) for \\(\\delta x \\rightarrow 0\\), then \\(p(x)\\) is called the probability density over \\(x\\). If \\(x\\) is a discrete variable, then \\(p(x)\\) is sometimes called a probability mass function. The probability density \\(p(x)\\) must satisfy two conditions \\[ \\begin{align} p(x) &amp;\\geq 0 \\\\ \\int_{-\\infty}^\\infty p(x) \\mathrm{d}x &amp;= 1 \\end{align} \\] One point worth noting is that under a nonlinear change of variable, a probability density transforms differently from a simple function replacement, but due to a Jacobian factor. Consider a change \\(x=g(y)\\), and \\(p_x(x)\\) and \\(p_y(y)\\) are different densities corresponding to each variables, since for some small values of \\(\\delta x\\), the observed values falling in the range \\((x, x+\\delta x)\\) will be transformed into the range \\((y, y + \\delta y)\\), or sometimes \\((y - \\delta y, y)\\), we have \\[ p_x(x)|\\delta x| \\simeq p_y(y) |\\delta y| \\] and hence \\[ \\begin{align} p_y(y) &amp;= p_x(x)\\left| \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\right| \\\\ &amp;= p_x(g(y))|g&#39;(y)| \\end{align} \\] Intuitively, the larger the derivative \\(g&#39;(y)\\) is, the sparser the changed distribution \\(p_x(x)\\) is. One consequence is that the maximum of a probability density is dependent on the choice of variable. Here \\(y = g^{-1}(x) =1/(1+exp(-x+5))\\) is a logistic sigmoid function, and \\(p_x(x)\\) is a Gaussian \\(\\mathcal{N}(6, 1)\\). The green curve stands for \\(p_x(g(y))\\), while the purple one stands for \\(p_x(g(y))|g&#39;(y)|\\), with a different maximum. Expectations and Covariances Most are mentioned in statistics class. Here a subscript is used when we consider expectations of functions of several variables to indicate which variable is being averaged over, e.g. below we only average over \\(x\\) \\[ \\mathbb{E}_x[f(x,y)] = \\sum_x p(x)f(x,y) \\] But personally I think this definition requires that \\(y\\) is irrelevant to \\(x\\), or just a parameter, otherwise it's somewhat nonsense if we multiply \\(f(x,y)\\), which considers a specific \\(y\\), with a marginal distribution \\(p(x)\\), which already integrated out the \\(y\\). Sometimes we also consider a conditional expectation, e.g. now we average \\(x\\) with \\(y\\) fixed \\[ \\mathbb{E}_x[f(x,y)|y] = \\sum_x p(x|y)f(x,y) \\] In the case of two random variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the covariance is a matrix \\[ \\begin{align} \\mathrm{cov} [\\mathbf{x,y}] &amp;= \\mathbb{E}_\\mathbf{x,y}[\\{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T - \\mathbb{E}[\\mathbf{y^T}]\\}] \\\\ &amp;= \\mathbb{E}_\\mathbf{x,y}[\\mathbf{xy}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^T] \\end{align} \\] If we consider the covariance of the components of \\(\\mathbf{x}\\) with each over, we denote \\(\\mathrm{cov}[\\mathbf{x}] \\equiv \\mathrm{cov}[\\mathbf{x},\\mathbf{x}]\\). Bayesian Probabilities Frequentists view probability as frequencies of random events, while Bayesian view probability as quantification of uncertainty. Given a data set \\(\\mathcal{D} = \\{t_1,...,t_n\\}\\) and model parameters \\(\\mathbf{w}\\), Bayes theorem takes the form \\[ p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})} \\] allowing us to evaluate uncertainty after \\(\\mathcal{D}\\) is observed. Here \\(p(\\mathbf{w}|\\mathcal{D})\\) is called the posterior probability, \\(p(\\mathcal{D}|\\mathbf{w})\\) can be viewed as a function of \\(\\mathbf{w}\\) called likelihood function, and \\(p(\\mathbf{w})\\) is prior probability where the prior knowledge is included. We can state the theorem in the form \\[ \\mathrm{posterior \\propto likelihood \\times prior} \\] By integrating both sides of the equation above, we have \\[ {p(\\mathcal{D})} = \\int p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})\\ \\mathrm{d}\\mathbf{w} \\] The likelihood function plays a central role in both Bayesian and frequentist paradigms. In a frequentist setting, \\(\\mathbf{w}\\) is being fixed, usually determined by some estimator like maximum likelihood, which is the same as minimizing the negative logarithm of it called error function. For example, we will later see that minimizing \\(E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2\\) is a form of likelihood maximization. While from the Bayesian viewpoint, there is only a single \\(\\mathcal{D}\\) and uncertainty is expressed through \\(p(\\mathbf{w})\\). Bootstrap is a frequentist way to evaluate accuracy, for example given a set \\(\\mathcal{D} = \\{1,2,3,4,5\\}\\), multi-datasets are generated like \\(\\{4,3,4,1,5\\}\\), \\(\\{1,3,5,5,1\\}\\), which can contain duplicates and evaluation is done on these new datasets. The Gaussian Distribution Univariate Gaussian distribution is defined by \\[ \\mathcal{N} (x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\mathrm{exp}\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\] Note that the reciprocal of the variance in this book is often written as \\(\\beta = 1/\\sigma^2\\) called the precision. Multivariate \\(D\\)-dimensional Gaussian distribution is defined by \\[ \\mathcal{N} (\\mathbf{x}|\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2} |\\mathbf{\\Sigma}|^{1/2}} \\mathrm{exp}\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\} \\] Suppose a dataset \\(\\mathtt{x} = (x_1,...,x_N)^T\\) sampled from a univariate Gaussian, i.i.d. between each of them (independently and identically distributed). So the probability of the data set given \\(\\mu\\) and \\(\\sigma^2\\) is \\[ p(\\mathtt x| \\mu , \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n| \\mu, \\sigma^2) \\] One way to determine the parameters is to maximize the likelihood function above, which can be changed into the logarithm form \\[ \\ln p(\\mathtt x| \\mu , \\sigma^2) = -\\frac{N}{2}\\ln (2\\pi) - N \\ln \\sigma - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 \\] By first differentiating it w.r.t. \\(\\mu\\) we get the solution of \\(\\mu\\) is just the sample mean \\[ \\mu_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N x_n \\] and then differentiating it w.r.t. \\(\\sigma^2\\) and use the solution of \\(\\mu\\) above we get the solution of \\(\\sigma^2\\) is just the sample variance \\[ \\sigma^2_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu_\\mathrm{ML})^2 \\] However, in fact, maximum likelihood approach underestimates the variance of the distribution \\[ \\mathbb{E} [\\sigma^2_\\mathrm{ML}] = \\frac{N-1}{N} \\sigma^2 \\] which can be easily proved by using \\(\\mathbb{E}[x_nx_m] = \\mu^2 + I_{nm}\\sigma^2\\), where \\(I_{nm}=1\\) if \\(n=m\\) and \\(I_{nm}=1\\) otherwise. Although the influence of underestimation will be less severe as \\(N\\) grows larger, however, more complex models with many parameters will still suffer. Bias arises in using maximum likelihood to determine the variance of a Gaussian. Curve Fitting Re-visited Previously we solved this problem by minimizing error, now from a probabilistic perspective we can gain some insights into error function, regularization and then a full Bayesian treatment. Now we first use a Gaussian to predict the distribution of data \\[ p(t|x,\\mathbf{w}, \\beta) = \\mathcal{N} (t| y(x,\\mathbf{w}), \\beta^{-1}) \\] Here \\(y(x,\\mathbf{w})= \\sum_{j=0}^M w_j x^j\\) is the polynomial function defined before, and \\(\\beta = 1/\\sigma^2\\) is called the precision of the Gaussian. Using maximum likelihood on the training data \\(\\{\\mathtt{x}, \\mathtt{t}\\}\\), the likelihood function is given by \\[ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) = \\prod_{n=1}^N\\mathcal{N} (t_n| y(x_n,\\mathbf{w}), \\beta^{-1}) \\] And we have \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\frac{1}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 \\end{aligned} \\] We can see that maximizing the log likelihood reach the same optimum as minimizing the sum-of squares error function. Further maximize w.r.t. \\(\\beta\\) we get \\[ \\frac{1}{\\beta_\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^N [y(x_n,\\mathbf{w}_\\mathrm{ML})-t_n]^2 \\] Now we have a probabilistic model expressed in terms of the predictive distribution by \\[ p(t|x,\\mathbf{w}_\\mathrm{ML}, \\beta_\\mathrm{ML}) = \\mathcal{N} (t| y(x,\\mathbf{w}_\\mathrm{ML}), \\beta_\\mathrm{ML}^{-1}) \\] For a more Bayesian approach, we introduce a prior distribution over \\(\\mathbf{w}\\) as Gaussian for simplicity \\[ p(\\mathbf{w}|\\alpha) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) = \\left(\\frac{\\alpha}{2\\pi}\\right)^{(M+1)/2} \\exp \\left\\{-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right\\} \\] Using Bayes theorem, \\[ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) = \\frac{p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)}{p(\\mathtt{t}|\\mathtt{x}, \\beta)} \\propto p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha) \\] To maximize the posterior probability w.r.t. \\(\\mathbf{w}\\), it is equivalent to \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln [p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)] \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}+ \\frac{M+1}{2}\\ln\\alpha - \\frac{M+1}{2}\\ln(2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w} \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 +\\frac{\\alpha}{\\beta}\\mathbf{w}^T\\mathbf{w} \\end{aligned} \\] This technique is called maximum posterior (MAP), and we can see that maximizing the posterior is equivalent to minimizing the regularized sum-of-square error with \\(\\lambda = \\alpha/\\beta\\). Bayesian Curve Fitting So far the predictive distribution gives a non-point estimate for \\(t\\), but it is predicted under a point estimate \\(\\mathbf{w}_\\mathrm{ML}\\), or \\(\\mathbf{w}_\\mathrm{MAP}\\). In a fully Bayesian approach, we should integrate all values of \\(\\mathbf{w}\\) \\[ p(t|x,\\mathtt{x},\\mathtt{t}) = \\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathtt{x},\\mathtt{t})\\mathrm{d}\\mathbf{w} \\] Here the precisions \\(\\alpha\\) and \\(\\beta\\) are assumed fixed and known in advance, which will discussed in section 3.3, together with the solution of the integral above.","link":"/2018/04/23/PRML1-2/"}],"tags":[{"name":"memo","slug":"memo","link":"/tags/memo/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"PRML","slug":"PRML","link":"/tags/PRML/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"ssr","slug":"ssr","link":"/tags/ssr/"}],"categories":[]}